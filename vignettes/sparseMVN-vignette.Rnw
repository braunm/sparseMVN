%\VignetteIndexEntry{Using the sparseMVN package}
\documentclass[11pt]{article}




\usepackage{amsmath}
\usepackage[small]{caption}
\usepackage{float}
\usepackage{placeins}

\usepackage[doublespacing]{setspace}
  \usepackage[sc]{mathpazo}
\usepackage{fancyvrb}
  \usepackage{graphicx}
  \newcommand{\pkg}[1]{\textbf{#1}}
  \usepackage{geometry} 
 \geometry{left=1.5in,right=1.5in,top=1.5in,bottom=1.5in}
 \usepackage{array}
  \usepackage[page]{appendix}
  \usepackage{xspace}
  \usepackage{parskip}
  \usepackage{natbib} 
  \DeclareMathOperator\Prob{Prob}
  \DeclareMathOperator\cov{cov}
  \bibpunct[, ]{(}{)}{;}{a}{}{,}
  \bibliographystyle{jss}
  \onehalfspacing
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\DefineVerbatimEnvironment{Code}{Verbatim}{}
\DefineVerbatimEnvironment{CodeInput}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{CodeOutput}{Verbatim}{}
\newenvironment{CodeChunk}{}{}

\newcommand{\func}[1]{\code{#1}}
\newcommand{\funcarg}[1]{\code{#1}}
\newcommand{\class}[1]{\texttt{#1}}
\newcommand{\method}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{#1}}
\newcommand{\methodType}{\tt}
\newcommand{\kron}{\otimes}

\title{sparseMVN: An R Package for Multivariate Normal Distributions with Sparse Covariance or Precision Matrices}
\author{Michael Braun\\SMU Cox School of Business\\Southern Methodist University}
\date{November 4, 2013}


\newcommand{\abstractText}{%
The \pkg{sparseMVN} package provides functions to sample from a multivariate normal distribution, and compute its density, when the covariance or precision matrix is sparse.  By exploiting this sparsity, we can handle high-dimensional distributions quickly, with lower memory usage.
}%

\begin{document}
\maketitle


\begin{abstract}\normalsize
\abstractText
\end{abstract}


\section{Introduction}

The \proglang{R} package \pkg{mvtnorm} \citep{R_mvtnorm} includes functions for sampling from a multivariate normal (MVN) distribution (\func{rmvnorm}), and for computing the density of an MVN sample (\func{dmvnorm}).  To use these functions, the user must provide the covariance matrix as a standard, base \proglang{R} matrix.  This approach can be inefficient when most of the entries in the covariance matrix are zero.  Not only will \proglang{R} store these zeros explicitly, but linear algebra functions (e.g., matrix multiplication, \func{solve}) will treat the zeros like any other number.  Additionally, \func{rmvnorm} computes a factorization (e.g., eigenvalue, singular value, or Cholesky) of this dense matrix every time the function is called.

The \pkg{sparseMVN} provides analogous functions to \func{rmvnorm} and \func{dmvnorm} that can be used when either the covariance or precision matrix is sparse.  Our method uses the classes and methods from the \pkg{Matrix} package \citep{R_Matrix}, which is now one of the ``recommended'' packages in \proglang{R}. In contrast to the \pkg{mvtnorm} functions, the user supplies the Cholesky decomposition of either the covariance or precision (inverse covariance) matrix.  By separating the Cholesky decomposition from the sampling or density functions, the user can call these functions repeatedly, while performing the factorization step only once.  Having the option to work with the precision matrix instead of the covariance avoids the need to explicitly invert the precision matrix when the latter is more readily available.  This option is useful, for example, when the precision of an MVN distribution is determined by the Hessian at the optimum of a target distribution, as in Laplace approximation of posterior densities.  More importantly, by using the sparse matrix algorithms in the \pkg{Matrix} package, we dramatically reduce the computational expense of sampling from, and computing densities of, MVN variates of very high dimension.

In the next section, we describe how \pkg{sparseMVN} works.  We then compare the functions in \pkg{sparseMVN} to those in \pkg{mvtnorm}, in terms of execution time.  We do not provide alternatives to any of the other functions in \pkg{mvtnorm}, such as those that estimate cumulative probabilities of the MVN, or those that deal with the multivariate t distribution.  In addition, we do not address the case for which the covariance matrix is not positive definite.

\section{Algorithmic details}

Let $X$ be a random variable, with $k$ dimensions, that is distributed MVN with mean $\mu$ and covariance $\Sigma$.  Let $L$ be a matrix root of $\Sigma$, such that $\Sigma=LL'$, and $L$ is lower triangular. To generate a sample realization $x$, we sample $k$ independent standard normal random variates (call that vector $z$), and let $x=\mu+Lz$.  The matrix factor $L$ could be generated via an eigenvalue, singular value, or Cholesky decomposition.  The Cholesky factor of a symmetric, positive definite matrix is the unique factor $L$ for which all of the diagonal elements are positive.  For the rest of this paper, we will use that definition for $L$.

The density of the MVN distribution is
\begin{align}
  \label{eq:MVNdens}  f(x)=(2\pi)^{-\frac{k}{2}}|\Sigma|^{-\frac{1}{2}}\exp\left[-\frac{1}{2}\left(x-\mu\right)'\Sigma^{-1}\left(x-\mu\right)\right]
\end{align}

The determinant of $\Sigma$ is the square of the product of the diagonal elements of $L$.  In addition, $\Sigma^{-1}=(LL')^{-1}=L'^{-1}L^{-1}$.  If we define $y=L^{-1}(x-\mu)$, we can write the log density of $x$ as
\begin{align}
  \label{eq:logDens1}
  \log f(x)=-\frac{k}{2}\log(2\pi)-\log|L|-\frac{1}{2}y'y
\end{align}

Since $L$ is triangular, we can solve $Ly=x-\mu$ for $y$ quickly, avoiding the need to invert $\Sigma$ explicitly.

In some cases, the precision matrix $\Sigma^{-1}$ is more readily available than $\Sigma$.  Let $\Sigma^{-1}=\Lambda\Lambda'$ represent the Cholesky decomposition of $\Sigma^{-1}$.  To sample $x$, we sample $z$ as before, solve $\Lambda'x=z$, and then add $\mu$.  Since $E(z)=0$ and $E(zz')=I_k$, we have $E(x)=\mu$, and $\cov(xx')=E(\Lambda'^{-1}zz'\Lambda^{-1})=\Lambda'^{-1}\Lambda^{-1}=(\Lambda\Lambda')^{-1}=\Sigma$.  Then, if we let $y=\Lambda'(x-\mu)$, the log density is 
\begin{align}
  \label{eq:logDens2}
  \log f(x)=-\frac{k}{2}\log(2\pi)+|\Lambda|-\frac{1}{2}y'y
\end{align}

Regardless of whether the user starts with $\Sigma$ or $\Sigma^{-1}$, either random sampling or density calculation will involve solving a triangular system. Which matrix to use depends mostly on convenience and the task at hand.

Nothing in this section so far is special for sparse $\Sigma$ or $\Sigma^{-1}$.  The efficiency gains in \pkg{sparseMVN} come from storing $\Sigma$ or $\Sigma^{-1}$ in a compressed format without explicit zeros, and applying linear algebra routines that are optimized for those sparse matrix structures.  The \pkg{Matrix} package calls sparse linear algebra routines that are implemented in the \pkg{CHOLMOD} library \citep{ChenDavis2008,DavisHager1999,DavisHager2009}; more information about these routines is available there.

\section{Using the package}

There are two functions in \pkg{sparseMVN}:  \func{rmvn.sparse} and \func{dmvn.sparse}.  The former samples from an MVN, and the latter computes the log density.  The signatures are

\begin{CodeInput}
rmvn.sparse(N, mu, CH, prec=TRUE)
dmvn.sparse(x, mu, CH, prec=TRUE)
\end{CodeInput}

\funcarg{N} is the number of samples to collect; each sample will be returned in a row in a standard base \proglang{R} matrix. Each row in the matrix \funcarg{x} is a sample for which we want to compute the log density.  In both functions, \funcarg{mu} is the mean (a vector), and \funcarg{CH} is the Cholesky decomposition either the covariance or precision matrix.  The argument \funcarg{prec} flags from which type of matrix \funcarg{CH} was decomposed.

\funcarg{CH} must be an object of the class \class{dCHMsimpl} or \class{dCHMsuper}, as returned by the \func{Cholesky} function in the \pkg{Matrix} package.  The first argument to \func{Cholesky} must be a sparse symmetric matrix, stored as an object of class \class{dsCMatrix}.  As far as we know, there is no particular need to deviate from the defaults of the remaining arguments.  If \func{Cholesky} uses a fill-reducing permutation to compute \funcarg{CH}, the \pkg{sparseMVN} functions will handle that directly, with no additional user intervention required.

A demonstration of how to use the package is in the file \file{demo/sparseMVN.R}, and can be run with default arguments by calling \texttt{demo(sparseMVN)}.  Much of the time in this script is spent constructing the covariance or precision matrix, and not in the sampling or computation routines.


\section{Timing comparison}

Next, we present the results of a timing comparison that demonstrates how much faster \func{rmvn.sparse} and \func{dmvn.sparse} can be than their \pkg{mvtnorm} counterparts.  In this example, we consider an MVN distribution for which either the covariance or precision matrix has a ``band-arrow'' structure.  To construct this matrix, we start with $Q_1$, a $p\times p$ dense, lower triangular matrix, where $p$ is a relatively small integer.  Then, let $Q_2=Q_1\kron I_m$, where $m$ can be large.  Note that $Q_2$ is also lower triangular.  Then, we augment $Q_2$ by appending $k$ additional rows, all of which are filled by random non-zero values.  Call this $(pm+k) \times (pm+k)$ lower triangular matrix $Q_3$.  Finally, we compute $Q=Q_3Q_3'$, which is symmetric, positive definite, and sparse. 

We can visualize the sparsity pattern of $Q$ as a $p\times p$ tiling of $m$-dimensional diagonal matrices, with dense margins on the bottom and the left.  Although $Q$ itself is $(pm+k)\times (pm+k)$, there are only $p^2m+2kpm+k^2$ non-zero elements.  For example, if $p=3$, $m=1000$, and $k=20$, $Q$ has 9,120,400, elements, but only 129,400 of them (1.4\%) are non-zero.  As $m$ increases, the matrix becomes more sparse.

The timing comparison consists of generating values for the covariance matrix, simulating 200 MVN draws, and computing the log densities, all for different values of  $p$ and $m$.  We replicate this study 30 times, and report the mean run times for random sampling and log density computation.  Then, we do the whole thing again, but treating the input matrix as a precision matrix instead.  Results are in Table \ref{tab:times}.  Computation was done on a 2010-vintage Mac Pro with 12 processing cores, each running at 2.93GHz, and with 32GB of RAM.

\begin{table}[ht]
\centering
\begin{tabular}{crr|rr|rr}
input&&&\multicolumn{2}{|c}{density eval}&\multicolumn{2}{|c}{200 mvn draws}\\
  matrix  & p & m & dense & sparse & dense & sparse \\ 
  \hline
 cov  & 2 & 25 & 0.01 & 0.01 & 0.01 & 0.02 \\ 
 cov  & 2 & 250 & 0.73 & 0.02 & 0.31 & 0.04 \\ 
  cov  & 2 & 500 & 1.62 & 0.03 & 0.45 & 0.08 \\ 
  cov  & 5 & 25 & 0.11 & 0.01 & 0.01 & 0.02 \\ 
  cov  & 5 & 250 & 3.63 & 0.05 & 1.15 & 0.12 \\ 
  cov  & 5 & 500 & 19.88 & 0.13 & 5.41 & 0.31 \\ 
  prec & 2 & 25 & 0.01 & 0.02 & 0.06 & 0.01 \\ 
  prec  & 2 & 250 & 0.84 & 0.02 & 0.25 & 0.03 \\ 
  prec  & 2 & 500 & 2.43 & 0.05 & 1.30 & 0.07 \\ 
  prec  & 5 & 25 & 0.12 & 0.01 & 0.02 & 0.01 \\ 
  prec  & 5 & 250 & 4.18 & 0.06 & 2.04 & 0.12 \\ 
  prec  & 5 & 500 & 21.44 & 0.13 & 7.52 & 0.27 \\ 
\end{tabular}
\caption{Time, in seconds, for evaluating the density of, and generating 200 random samples from, a multivariate normal distribution.  The input matrix is either a covariance or precision, and is either dense or sparse.  If dense, routines from \pkg{mvtnorm} are used.  Otherwise, results are from \pkg{sparseMVN}.  For all examples here, $k=15$.}
\label{tab:times}
\end{table}

For small $m$, the MVN routines in \pkg{sparseMVN}, actually take longer than those in \pkg{mvtnorm}.  But as either the precision or covariance matrices become more sparse, we see a dramatic speed-up in computation time.

Table \ref{tab:times} was generated from the file \file{inst/tables.R} in the package source code.  Users can generate one replication of the comparison by calling \texttt{demo(sparseMVN)}.

\FloatBarrier

\bibliography{sparseMVN}

\end{document}