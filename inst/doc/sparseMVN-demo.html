<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Michael Braun" />

<meta name="date" content="2015-02-04" />

<title>Using sparseMVN</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link href="data:text/css,body%20%7B%0A%20%20background%2Dcolor%3A%20%23fff%3B%0A%20%20margin%3A%201em%20auto%3B%0A%20%20max%2Dwidth%3A%20700px%3B%0A%20%20overflow%3A%20visible%3B%0A%20%20padding%2Dleft%3A%202em%3B%0A%20%20padding%2Dright%3A%202em%3B%0A%20%20font%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0A%20%20font%2Dsize%3A%2014px%3B%0A%20%20line%2Dheight%3A%201%2E35%3B%0A%7D%0A%0A%23header%20%7B%0A%20%20text%2Dalign%3A%20center%3B%0A%7D%0A%0A%23TOC%20%7B%0A%20%20clear%3A%20both%3B%0A%20%20margin%3A%200%200%2010px%2010px%3B%0A%20%20padding%3A%204px%3B%0A%20%20width%3A%20400px%3B%0A%20%20border%3A%201px%20solid%20%23CCCCCC%3B%0A%20%20border%2Dradius%3A%205px%3B%0A%0A%20%20background%2Dcolor%3A%20%23f6f6f6%3B%0A%20%20font%2Dsize%3A%2013px%3B%0A%20%20line%2Dheight%3A%201%2E3%3B%0A%7D%0A%20%20%23TOC%20%2Etoctitle%20%7B%0A%20%20%20%20font%2Dweight%3A%20bold%3B%0A%20%20%20%20font%2Dsize%3A%2015px%3B%0A%20%20%20%20margin%2Dleft%3A%205px%3B%0A%20%20%7D%0A%0A%20%20%23TOC%20ul%20%7B%0A%20%20%20%20padding%2Dleft%3A%2040px%3B%0A%20%20%20%20margin%2Dleft%3A%20%2D1%2E5em%3B%0A%20%20%20%20margin%2Dtop%3A%205px%3B%0A%20%20%20%20margin%2Dbottom%3A%205px%3B%0A%20%20%7D%0A%20%20%23TOC%20ul%20ul%20%7B%0A%20%20%20%20margin%2Dleft%3A%20%2D2em%3B%0A%20%20%7D%0A%20%20%23TOC%20li%20%7B%0A%20%20%20%20line%2Dheight%3A%2016px%3B%0A%20%20%7D%0A%0Atable%20%7B%0A%20%20margin%3A%201em%20auto%3B%0A%20%20border%2Dwidth%3A%201px%3B%0A%20%20border%2Dcolor%3A%20%23DDDDDD%3B%0A%20%20border%2Dstyle%3A%20outset%3B%0A%20%20border%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0A%20%20border%2Dwidth%3A%202px%3B%0A%20%20padding%3A%205px%3B%0A%20%20border%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0A%20%20border%2Dwidth%3A%201px%3B%0A%20%20border%2Dstyle%3A%20inset%3B%0A%20%20line%2Dheight%3A%2018px%3B%0A%20%20padding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0A%20%20border%2Dleft%2Dstyle%3A%20none%3B%0A%20%20border%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0A%20%20background%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0A%0Ap%20%7B%0A%20%20margin%3A%200%2E5em%200%3B%0A%7D%0A%0Ablockquote%20%7B%0A%20%20background%2Dcolor%3A%20%23f6f6f6%3B%0A%20%20padding%3A%200%2E25em%200%2E75em%3B%0A%7D%0A%0Ahr%20%7B%0A%20%20border%2Dstyle%3A%20solid%3B%0A%20%20border%3A%20none%3B%0A%20%20border%2Dtop%3A%201px%20solid%20%23777%3B%0A%20%20margin%3A%2028px%200%3B%0A%7D%0A%0Adl%20%7B%0A%20%20margin%2Dleft%3A%200%3B%0A%7D%0A%20%20dl%20dd%20%7B%0A%20%20%20%20margin%2Dbottom%3A%2013px%3B%0A%20%20%20%20margin%2Dleft%3A%2013px%3B%0A%20%20%7D%0A%20%20dl%20dt%20%7B%0A%20%20%20%20font%2Dweight%3A%20bold%3B%0A%20%20%7D%0A%0Aul%20%7B%0A%20%20margin%2Dtop%3A%200%3B%0A%7D%0A%20%20ul%20li%20%7B%0A%20%20%20%20list%2Dstyle%3A%20circle%20outside%3B%0A%20%20%7D%0A%20%20ul%20ul%20%7B%0A%20%20%20%20margin%2Dbottom%3A%200%3B%0A%20%20%7D%0A%0Apre%2C%20code%20%7B%0A%20%20background%2Dcolor%3A%20%23f7f7f7%3B%0A%20%20border%2Dradius%3A%203px%3B%0A%20%20color%3A%20%23333%3B%0A%7D%0Apre%20%7B%0A%20%20white%2Dspace%3A%20pre%2Dwrap%3B%20%20%20%20%2F%2A%20Wrap%20long%20lines%20%2A%2F%0A%20%20border%2Dradius%3A%203px%3B%0A%20%20margin%3A%205px%200px%2010px%200px%3B%0A%20%20padding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0A%20%20background%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0A%0Acode%20%7B%0A%20%20font%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0A%20%20font%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0A%20%20padding%3A%202px%200px%3B%0A%7D%0A%0Adiv%2Efigure%20%7B%0A%20%20text%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0A%20%20background%2Dcolor%3A%20%23FFFFFF%3B%0A%20%20padding%3A%202px%3B%0A%20%20border%3A%201px%20solid%20%23DDDDDD%3B%0A%20%20border%2Dradius%3A%203px%3B%0A%20%20border%3A%201px%20solid%20%23CCCCCC%3B%0A%20%20margin%3A%200%205px%3B%0A%7D%0A%0Ah1%20%7B%0A%20%20margin%2Dtop%3A%200%3B%0A%20%20font%2Dsize%3A%2035px%3B%0A%20%20line%2Dheight%3A%2040px%3B%0A%7D%0A%0Ah2%20%7B%0A%20%20border%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0A%20%20padding%2Dtop%3A%2010px%3B%0A%20%20padding%2Dbottom%3A%202px%3B%0A%20%20font%2Dsize%3A%20145%25%3B%0A%7D%0A%0Ah3%20%7B%0A%20%20border%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0A%20%20padding%2Dtop%3A%2010px%3B%0A%20%20font%2Dsize%3A%20120%25%3B%0A%7D%0A%0Ah4%20%7B%0A%20%20border%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0A%20%20margin%2Dleft%3A%208px%3B%0A%20%20font%2Dsize%3A%20105%25%3B%0A%7D%0A%0Ah5%2C%20h6%20%7B%0A%20%20border%2Dbottom%3A%201px%20solid%20%23ccc%3B%0A%20%20font%2Dsize%3A%20105%25%3B%0A%7D%0A%0Aa%20%7B%0A%20%20color%3A%20%230033dd%3B%0A%20%20text%2Ddecoration%3A%20none%3B%0A%7D%0A%20%20a%3Ahover%20%7B%0A%20%20%20%20color%3A%20%236666ff%3B%20%7D%0A%20%20a%3Avisited%20%7B%0A%20%20%20%20color%3A%20%23800080%3B%20%7D%0A%20%20a%3Avisited%3Ahover%20%7B%0A%20%20%20%20color%3A%20%23BB00BB%3B%20%7D%0A%20%20a%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0A%20%20%20%20text%2Ddecoration%3A%20underline%3B%20%7D%0A%20%20a%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0A%20%20%20%20text%2Ddecoration%3A%20underline%3B%20%7D%0A%0A%2F%2A%20Class%20described%20in%20https%3A%2F%2Fbenjeffrey%2Ecom%2Fposts%2Fpandoc%2Dsyntax%2Dhighlighting%2Dcss%0A%20%20%20Colours%20from%20https%3A%2F%2Fgist%2Egithub%2Ecom%2Frobsimmons%2F1172277%20%2A%2F%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%2F%2A%20Keyword%20%2A%2F%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%2F%2A%20DataType%20%2A%2F%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%2F%2A%20DecVal%20%28decimal%20values%29%20%2A%2F%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20BaseN%20%2A%2F%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20Float%20%2A%2F%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20Char%20%2A%2F%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20String%20%2A%2F%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%2F%2A%20Comment%20%2A%2F%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%2F%2A%20OtherToken%20%2A%2F%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%2F%2A%20AlertToken%20%2A%2F%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%2F%2A%20Function%20calls%20%2A%2F%20%0Acode%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%2F%2A%20ErrorTok%20%2A%2F%0A%0A" rel="stylesheet" type="text/css" />

</head>

<body>



<div id="header">
<h1 class="title">Using sparseMVN</h1>
<h4 class="author"><em>Michael Braun</em></h4>
<h4 class="date"><em>2015-02-04</em></h4>
</div>


<p>The <em>sparseMVN</em> package provides functions to sample from a multivariate normal (MVN) distribution, and compute its density, when the covariance or precision matrix is sparse. By exploiting this sparsity, we can reduce the amount of computational resources that are needed for matrix storage, and for linear algebra routines like matrix-vector multiplication, solving linear systems, and computing Cholesky factors. Sparse matrix structures store only the row and column indices, and the values, of the non-zero elements of the matrix. All other elements are assumed to be zero, so they do not need to be stored explicitly. Many linear algebra libraries, such as those called by the <em>Matrix</em> package, include routines that are optimized for sparse matrices. These routines are much faster than their dense matrix counterparts because they effectively skip over atomic operations that involve the zeros in the matrices.</p>
<div id="background" class="section level2">
<h2>Background</h2>
<div id="what-makes-a-matrix-sparse" class="section level3">
<h3>What makes a matrix “sparse”</h3>
<p>By “sparse matrix,” we mean a matrix for which relatively few elements are non-zero. One situation in which sparse covariance/precision matrices occur is when sampling from a normal approximation to a posterior density of a hierarchical model. For example, suppose we have <span class="math">\(N\)</span> heterogeneous units, each with a parameter vector of length <span class="math">\(k\)</span>. Also, suppose that there are <span class="math">\(p\)</span> population-level parameters (e.g., parameters on hyperpriors, homogeneous coefficients, etc). If we assume that the <span class="math">\(N\)</span> parameter vectors are conditionally independent across units, then the cross-partial derivatives of elements of different vectors is zero.</p>
<p>If <span class="math">\(N=5,~k= 2\)</span>, and <span class="math">\(p= 2\)</span>, then there are 12 total variables, and the Hessian will have the following “block-arrow” pattern.</p>
<pre><code># 12 x 12 sparse Matrix of class &quot;lgCMatrix&quot;
#                              
#  [1,] | | . . . . . . . . | |
#  [2,] | | . . . . . . . . | |
#  [3,] . . | | . . . . . . | |
#  [4,] . . | | . . . . . . | |
#  [5,] . . . . | | . . . . | |
#  [6,] . . . . | | . . . . | |
#  [7,] . . . . . . | | . . | |
#  [8,] . . . . . . | | . . | |
#  [9,] . . . . . . . . | | | |
# [10,] . . . . . . . . | | | |
# [11,] | | | | | | | | | | | |
# [12,] | | | | | | | | | | | |</code></pre>
<p>There are 144 elements in this symmetric matrix, but only 64 are non-zero, and only 38 values are unique. Although the reduction in RAM from using a sparse matrix structure for the Hessian may be modest, consider what would happen if <span class="math">\(N=1000\)</span> instead. In that case, there are 2002 variables in the problem, and more than <span class="math">\(4\)</span> million elements in the Hessian. However, only <span class="math">\(12004\)</span> of those elements are non-zero. If we work with only the lower triangle of the Hessian we only need to work with only 7003 unique values.</p>
<p>Since a large proportion of elements in the matrix are zero, we need to store only the row and column indices, and the values, of the unique non-zero elements. The efficiency gains in <em>sparseMVN</em> come from storing the covariance or precision matrix in a compressed format without explicit zeros, and applying linear algebra routines that are optimized for those sparse matrix structures. The <em>Matrix</em> package calls sparse linear algebra routines that are implemented in the <em>CHOLMOD</em> library <span class="citation">(Chen et al. 2008; Davis and Hager 1999; Davis and Hager 2009)</span>; more information about these routines is available there.</p>
</div>
<div id="why-linear-algebra-matters-for-mvn-random-variables" class="section level3">
<h3>Why linear algebra matters for MVN random variables</h3>
<p>To see why using linear algebra algorithms that are optimized for sparse matrices is useful, let’s first look at how one would, in general, sample from an MVN. Let <span class="math">\(X\)</span> be a random variable, with <span class="math">\(q\)</span> dimensions, that is distributed MVN with mean <span class="math">\(\mu\)</span> and covariance <span class="math">\(\Sigma\)</span>. Let <span class="math">\(L\)</span> be a lower triangular matrix root of <span class="math">\(\Sigma\)</span>, such that <span class="math">\(\Sigma=LL'\)</span>. To generate a sample from <span class="math">\(X\)</span>, we sample <span class="math">\(q\)</span> independent standard normal random variates (call that vector <span class="math">\(z\)</span>), and let <span class="math">\(x=\mu+Lz\)</span>. The matrix factor <span class="math">\(L\)</span> could be generated via an eigenvalue, singular value, or Cholesky decomposition. The Cholesky factor of a symmetric, positive definite matrix is the unique factor <span class="math">\(L\)</span> for which all of the diagonal elements are positive. For the rest of this paper, we will use that definition for <span class="math">\(L\)</span>.</p>
<p>There are three linear algebra operations involved:</p>
<ol style="list-style-type: decimal">
<li>Decomposing <span class="math">\(\Sigma=LL'\)</span>;</li>
<li>Multiplying the triangular matrix <span class="math">\(L\)</span> by vector <span class="math">\(z\)</span>; and</li>
<li>Adding <span class="math">\(\mu\)</span> to <span class="math">\(Lz\)</span>.</li>
</ol>
<p>The <code>rmvnorm</code> function in the <em>mvtnorm</em> package <span class="citation">(Genz et al. 2012)</span> uses this algorithm, taking <span class="math">\(\Sigma\)</span>, as a base <strong>R</strong> matrix, as one of the arguments. <em>Each call</em> to <code>rmvnorm</code> factors <span class="math">\(\Sigma\)</span>. If <span class="math">\(\Sigma\)</span> is stored as a typical, base <strong>R</strong> dense matrix, then the computation time for a Cholesky decomposition grows <em>cubicly</em>, and the time to multiply <span class="math">\(L\)</span> and <span class="math">\(z\)</span> grows quadratically, with the dimension of <span class="math">\(X\)</span> <span class="citation">(Golub and Van Loan 1996)</span>. Thus, <code>rmvnorm</code> can be quite resource-intensive for large <span class="math">\(X\)</span>, especially if the function is called repeatedly.</p>
<p>Computing the density of MVN draws faces similar scalability problems. The density of the MVN distribution is <span class="math">\[
f(x)=(2\pi)^{-\frac{k}{2}}|\Sigma|^{-\frac{1}{2}}\exp\left[-\frac{1}{2}\left(x-\mu\right)'\Sigma^{-1}\left(x-\mu\right)\right]
\]</span></p>
<p><span class="math">\(\Sigma^{-1}=(LL')^{-1}=L'^{-1}L^{-1}\)</span>. If we define <span class="math">\(y=L^{-1}(x-\mu)\)</span>, we can write the log density of <span class="math">\(x\)</span> as <span class="math">\[
  \log f(x)=-\frac{k}{2}\log(2\pi)-\log|L|-\frac{1}{2}y'y
\]</span></p>
<p>As with generating MVN samples, computing the density requires a matrix factorization that, as above, grows in complexity at a rate cubic to <span class="math">\(q\)</span>. The determinant of <span class="math">\(\Sigma\)</span> is the square of the product of the diagonal elements of <span class="math">\(L\)</span>. Computing <span class="math">\(y\)</span> involves solving a triangular linear system <span class="math">\(Ly=(x-\mu)\)</span>, the complexity of which grows quadratically with <span class="math">\(q\)</span>. The <code>dmvnorm</code> function in <em>mvtnorm</em> computes the density this way, so, like <code>rmvnorm</code>, it is resource-intensive for high dimensional <span class="math">\(X\)</span>.</p>
<p>The exact amount of time and memory that are saved by saving the covariance/precision matrix in a sparse format depends on the sparsity pattern. But for the hierarchical model example from earlier in this section, the number of non-zero elements grows only linearly with <span class="math">\(N\)</span>. The result is that all of the steps of sampling from an MVN also grow linearly with <span class="math">\(N\)</span>. Section 4 of <span class="citation">Braun and Damien (2014)</span> explains why this is so.</p>
</div>
</div>
<div id="using-the-sparsemvn-package" class="section level2">
<h2>Using the sparseMVN package</h2>
<p>The <em>sparseMVN</em> package provides <code>rmvn.sparse</code> and <code>dmvn.sparse</code> as alternatives to <code>rmvnorm</code> and <code>dmvnorm</code>. The signatures are</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rmvn.sparse</span>(ndraws, mu, CH, <span class="dt">prec=</span><span class="ot">TRUE</span>)
<span class="kw">dmvn.sparse</span>(x, mu, CH, <span class="dt">prec=</span><span class="ot">TRUE</span>)</code></pre>
<p><code>rmvn.sparse</code> returns a matrix <span class="math">\(X\)</span> with each sample in a row. The mean vector <span class="math">\(\mu\)</span> has <span class="math">\(q\)</span> elements. CH is either a <code>dCHMsimpl</code> or <code>dCHMsuper</code> object, and is computed using the <code>Cholesky</code> function in the <em>Matrix</em> package. The <code>prec</code> argument identifies if CH is the decomposition of either the covariance (<span class="math">\(\Sigma\)</span>, <code>prec=FALSE</code>) or precision (<span class="math">\(\Sigma^{-1}\)</span>, <code>prec=TRUE</code>) matrix. These functions do require the user to compute the Cholesky decomposition beforehand, but this needs to be done only once (as long as <span class="math">\(\Sigma\)</span> does not change).</p>
<p>More details about the <code>Cholesky</code> function are available in the <em>Matrix</em> package, but it is a simple function to use. The first argument is a sparse symmetric Matrix stored as a <code>dsCMatrix</code> object. As far as we know, there is no particular need to deviate from the defaults of the remaining arguments. If <code>Cholesky</code> uses a fill-reducing permutation to compute <code>CH</code>, the functions in <em>sparseMVN</em> will handle that directly, with no additional user intervention required.</p>
<p>Do not use the <code>chol</code> function in base <strong>R</strong>. <code>Cholesky</code> is designed for decomposing <em>sparse</em> matrices, which involves permuting rows and columns to maintain sparsity of the factor. The <code>dCHMsimple</code> and <code>dCHMsuper</code> objects store this permutation, which <code>rmvn.sparse</code> and <code>dmvn.sparse</code> need.</p>
<div id="an-example" class="section level3">
<h3>An example</h3>
<p>Suppose we want to generate <span class="math">\(R\)</span> samples from <span class="math">\(X\)</span>, where <span class="math">\(X\sim MVN(\mu,\Sigma)\)</span>, and the structure of <span class="math">\(\Sigma\)</span> is defined by our example model. In the following code, we first construct the mean vector <span class="math">\(\mu\)</span>. Then, we construct a random “block arrow” covariance matrix for a given <span class="math">\(N\)</span>, <span class="math">\(p\)</span>, and <span class="math">\(k\)</span>. We use functions from the <em>Matrix</em> package to ensure that <span class="math">\(\Sigma\)</span> is stored as a sparse matrix in compressed format.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(Matrix)
<span class="kw">set.seed</span>(<span class="dv">123</span>)
N &lt;-<span class="st"> </span><span class="dv">5</span>  ## number of blocks in sparse covariance matrix
p &lt;-<span class="st"> </span><span class="dv">2</span> ## size of each block
k &lt;-<span class="st"> </span><span class="dv">2</span>  ##
R &lt;-<span class="st"> </span><span class="dv">10</span>
    
## mean vector
mu &lt;-<span class="st"> </span><span class="kw">seq</span>(-<span class="dv">3</span>,<span class="dv">3</span>,<span class="dt">length=</span>k*N+p)

## build random block-arrow covariance/precision matrix for test
Q1 &lt;-<span class="st"> </span><span class="kw">tril</span>(<span class="kw">kronecker</span>(<span class="kw">diag</span>(N), <span class="kw">Matrix</span>(<span class="kw">seq</span>(<span class="fl">0.1</span>,<span class="fl">1.1</span>,<span class="dt">length=</span>k*k),k,k)))
Q2 &lt;-<span class="st"> </span><span class="kw">Matrix</span>(<span class="kw">rnorm</span>(N*k*p), p, N*k)
Q3 &lt;-<span class="st"> </span><span class="kw">tril</span>(<span class="fl">0.2</span>*<span class="kw">diag</span>(p))
Sigma &lt;-<span class="st"> </span><span class="kw">rBind</span>(<span class="kw">cBind</span>(Q1, <span class="kw">Matrix</span>(<span class="dv">0</span>, N*k, p)), <span class="kw">cBind</span>(Q2, Q3))
Sigma &lt;-<span class="st"> </span>Matrix::<span class="kw">tcrossprod</span>(Sigma)
<span class="kw">class</span>(Sigma)
<span class="co"># [1] &quot;dsCMatrix&quot;</span>
<span class="co"># attr(,&quot;package&quot;)</span>
<span class="co"># [1] &quot;Matrix&quot;</span></code></pre>
<p>Next, we compute the Cholesky decomposition of <span class="math">\(\Sigma\)</span> using the <code>Cholesky</code> function, and call <code>rmvn.sparse</code>, noting that <code>chol.Sigma</code> is the decomposition of a <em>covariance</em> matrix, and not a precision matrix.</p>
<pre class="sourceCode r"><code class="sourceCode r">chol.Sigma &lt;-<span class="st"> </span>Matrix::<span class="kw">Cholesky</span>(Sigma)  ## creates a dCHMsimpl object
x &lt;-<span class="st"> </span><span class="kw">rmvn.sparse</span>(R, mu, chol.Sigma, <span class="dt">prec=</span><span class="ot">FALSE</span>)</code></pre>
<p>Each row of <code>x</code> is a sample, and each column is a variable.</p>
<p>The <code>dmvn.sparse</code> function returns the log density for each row. Since we have already computed the Cholesky decomposition for <span class="math">\(\Sigma\)</span>, we do not need to do it again.</p>
<pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span><span class="kw">dmvn.sparse</span>(x, mu, chol.Sigma, <span class="dt">prec=</span><span class="ot">FALSE</span>)
<span class="kw">str</span>(d)
<span class="co">#  num [1:10] -1.707 -1.823 -0.775 0.332 -4.417 ...</span></code></pre>
<p>Sometimes the precision matrix <span class="math">\(\Sigma^{-1}\)</span> is more readily available than <span class="math">\(\Sigma\)</span>. For example, the negative Hessian of a log posterior density at the posterior mode is the precision of the normal approximation to that density. Let <span class="math">\(\Sigma^{-1}=\Lambda\Lambda'\)</span> represent the Cholesky decomposition of <span class="math">\(\Sigma^{-1}\)</span>. To sample <span class="math">\(x\)</span>, we sample <span class="math">\(z\)</span> as before, solve <span class="math">\(\Lambda'x=z\)</span>, and then add <span class="math">\(\mu\)</span>. Since <span class="math">\(E(z)=0\)</span> and <span class="math">\(E(zz')=I_k\)</span>, we have <span class="math">\(E(x)=\mu\)</span>, and <span class="math">\(\cov(xx')=E(\Lambda'^{-1}zz'\Lambda^{-1})=\Lambda'^{-1}\Lambda^{-1}=(\Lambda\Lambda')^{-1}=\Sigma\)</span>. Then, if we let <span class="math">\(y=\Lambda'(x-\mu)\)</span>, the log density is <span class="math">\[
  \log f(x)=-\frac{k}{2}\log(2\pi)+|\Lambda|-\frac{1}{2}y'y
\]</span></p>
<p>By setting the argument <code>prec=TRUE</code>, <code>rmvn.sparse</code> and <code>dmvn.sparse</code> will recognize the Cholesky decomposition as being for a <em>precision</em> matrix instead of a covariance matrix. Without this option, the user would need to explicitly invert <span class="math">\(\Sigma\)</span> beforehand. Even if $Sigma^{-1}* were sparse, there is no guarantee that <span class="math">\(\Sigma\)</span> would be (and vice versa). <code>rmvn.sparse</code> and <code>dmvn.sparse</code> let the user work with either the covariance or precision matrix, depending on which is more convenient.</p>
</div>
</div>
<div id="timing" class="section level2">
<h2>Timing</h2>
<p>A timing comparison is available as a separate vignette.</p>
</div>
<div id="other-packages-for-creating-and-using-sparse-matrices" class="section level2">
<h2>Other packages for creating and using sparse matrices</h2>
<div id="sparsehessianfd" class="section level3">
<h3>sparseHessianFD</h3>
<p>Suppose you have a objective function that has a sparse Hessian (e.g., the log posterior density for a hierarchical model). You have an <strong>R</strong> function that computes the value of the objective, and another function that computes its gradient. You may also need the Hessian, either for a nonlinear optimization routine, or as the negative precision matrix of an MVN approximation.</p>
<p>It’s hard enough to get the gradient, but the derivation of the Hessian might be too tedious or complicated for it to be worthwhile. However, it should not be too hard to identify which elements of the Hessian are non-zero. If you have both the gradient, and the Hessian <em>pattern</em>, then you can use the <em>sparseHessianFD</em> package <span class="citation">(Braun 2015)</span>to estimate the Hessian itself.</p>
<p>The <em>sparseHessianFD</em> package estimates the Hessian numerically, but in a way that exploits the fact that the Hessian is sparse, and that the pattern is known. The package contains functions that return the Hessian as a sparse <code>dgCMatrix</code>. This object can be coerced into a <code>dsCMatrix</code>, which in turn can be used by <code>rmvn.sparse</code> and <code>dmvn.sparse</code>.</p>
</div>
<div id="trustoptim" class="section level3">
<h3>trustOptim</h3>
<p>The <em>trustOptim</em> package provides a nonlinear optimization routine that takes the Hessian as a sparse <code>dgCMatrix</code> object. This optimizer is useful for unconstrained optimization of a high-dimensional objective function with a sparse Hessian. It uses a trust region algorithm, which may be more stable and robust than line search approaches. Also, it applies a stopping rule based on the norm of the gradient, as opposed to whether the algorithm makes “sufficient progress.” (Many optimizers, especially <code>optim</code> in base <strong>R</strong>, stop too early, before the gradient is truly flat).</p>
<div class="references">
<h2>References</h2>
<p>Braun, Michael. 2015. <em>sparseHessianFD: An R Package for Estimating Sparse Hessians</em> (version 0.2.0). R package. <a href="http://cran.r-project.org/package=sparseHessianFD" class="uri">http://cran.r-project.org/package=sparseHessianFD</a>.</p>
<p>Braun, Michael, and Paul Damien. 2014. <em>Scalable Rejection Sampling for Bayesian Hierarchical Models</em>. <a href="http://arxiv.org/abs/1401.8236" class="uri">http://arxiv.org/abs/1401.8236</a>.</p>
<p>Chen, Yanqing, Timothy A Davis, William W Hager, and Sivasankaran Rajamanickam. 2008. “Algorithm 887: CHOLMOD, Supernodal Sparse Cholesky Factorization and Update/Downdate.” <em>ACM Transactions on Mathematical Software</em> 35 (3): 1–14. doi:<a href="http://dx.doi.org/10.1145/1391989.1391995">10.1145/1391989.1391995</a>.</p>
<p>Davis, Timothy A, and William W Hager. 1999. “Modifying a Sparse Cholesky Factorization.” <em>SIAM Journal on Matrix Analysis and Applications</em> 20 (3): 606–27. doi:<a href="http://dx.doi.org/10.1137/S0895479897321076">10.1137/S0895479897321076</a>.</p>
<p>———. 2009. “Dynamic Supernodes in Sparse Cholesky Update/Downdate and Triangular Solves.” <em>ACM Transactions on Mathematical Software</em> 35 (4): 1–23. doi:<a href="http://dx.doi.org/10.1145/1462173.1462176">10.1145/1462173.1462176</a>.</p>
<p>Genz, Alan, Frank Bretz, Tetsuhisa Miwa, Xuefei Mi, Friedrich Leisch, Fabian Scheipl, and Torsten Hothorn. 2012. <em>mvtnorm: Multivariate Normal and T Distributions</em> (version 0.9-9994). R package.</p>
<p>Golub, Gene H, and Charles F Van Loan. 1996. <em>Matrix Computations</em>. 3rd ed. Johns Hopkins University Press.</p>
</div>
</div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
