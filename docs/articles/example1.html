<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>An example with a sparse Hessian • sparseMVN</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/spacelab/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="An example with a sparse Hessian">
<meta property="og:description" content="sparseMVN">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-9Q2M3WL4Z1"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9Q2M3WL4Z1');
</script>
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">sparseMVN</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.2.2</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/example1.html">An example with a sparse Hessian</a>
    </li>
    <li>
      <a href="../articles/theory1.html">sparseMVN: The Theory</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/braunm/sparseMVN/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="example1_files/header-attrs-2.11/header-attrs.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>An example with a sparse Hessian</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/braunm/sparseMVN/blob/master/vignettes/example1.Rmd" class="external-link"><code>vignettes/example1.Rmd</code></a></small>
      <div class="hidden name"><code>example1.Rmd</code></div>

    </div>

    
    
<div id="sec:sparse" class="section level2">
<h2 class="hasAnchor">
<a href="#sec:sparse" class="anchor" aria-hidden="true"></a>Sparse matrices in R</h2>
<p>The <a href="https://cran.r-project.org/package=Matrix" class="external-link">Matrix</a> package defines various classes for storing sparse matrices in compressed formats. The most important class for our purposes is <code>dsCMatrix</code>, which defines a symmetric matrix, with numeric (double precision) elements, in a column-compressed format. Three vectors define the underlying matrix: the unique nonzero values (just one triangle is needed), the indices in the value vector for the first value in each column, and the indices of the rows in which each value is located. The storage requirements for a sparse <span class="math inline">\(M\times M\)</span> symmetric matrix with <span class="math inline">\(V\)</span> unique nonzero elements in one triangle are for <span class="math inline">\(V\)</span> double precision numbers, <span class="math inline">\(V+M+1\)</span> integers, and some metadata. In contrast, a dense representation of the same matrix stores <span class="math inline">\(M^2\)</span> double precision values, regardless of symmetry and the number of zeros. If <span class="math inline">\(V\)</span> grows more slowly than <span class="math inline">\(M^2\)</span>, the matrix becomes increasingly sparse (a smaller percentage of elements are nonzero), with greater efficiency gains from storing the matrix in a compressed sparse format.</p>
<div id="an-example" class="section level3">
<h3 class="hasAnchor">
<a href="#an-example" class="anchor" aria-hidden="true"></a>An example</h3>
<p>To illustrate how sparse matrices require less memory resources when compressed than when stored densely, consider the following example, which borrows heavily from the vignette of the <a href="braunm.github.io/sparseHessianFD">sparseHessianFD</a> package.</p>
<p>Suppose we have a dataset of <span class="math inline">\(N\)</span> households, each with <span class="math inline">\(T\)</span> opportunities to purchase a particular product. Let <span class="math inline">\(y_i\)</span> be the number of times household <span class="math inline">\(i\)</span> purchases the product, out of the <span class="math inline">\(T\)</span> purchase opportunities, and let <span class="math inline">\(p_i\)</span> be the probability of purchase. The heterogeneous parameter <span class="math inline">\(p_i\)</span> is the same for all <span class="math inline">\(T\)</span> opportunities, so <span class="math inline">\(y_i\)</span> is a binomial random variable.</p>
<p>Let <span class="math inline">\(\beta_i\in\mathbb{R}^{k}\)</span> be a heterogeneous coefficient vector that is specific to household <span class="math inline">\(i\)</span>, such that <span class="math inline">\(\beta_i=(\beta_{i1},\dotsc,\beta_{ik})\)</span>. Similarly, <span class="math inline">\(w_i\in\mathbb{R}^{k}\)</span> is a vector of household-specific covariates. Define each <span class="math inline">\(p_i\)</span> such that the log odds of <span class="math inline">\(p_i\)</span> is a linear function of <span class="math inline">\(\beta_i\)</span> and <span class="math inline">\(w_i\)</span>, but does not depend directly on <span class="math inline">\(\beta_j\)</span> and <span class="math inline">\(w_j\)</span> for another household <span class="math inline">\(j\neq i\)</span>. <span class="math display">\[\begin{aligned}
  p_i=\frac{\exp(w_i'\beta_i)}{1+\exp(w_i'\beta_i)},~i=1 ... N\end{aligned}\]</span></p>
<p>The coefficient vectors <span class="math inline">\(\beta_i\)</span> are distributed across the population of households following a MVN distribution with mean <span class="math inline">\(\mu\in\mathbb{R}^{k}\)</span> and covariance <span class="math inline">\(\mathbf{A}\in\mathbb{R}^{k\times k}\)</span>. Assume that we know <span class="math inline">\(\mathbf{A}\)</span>, but not <span class="math inline">\(\mu\)</span>, so we place a multivariate normal prior on <span class="math inline">\(\mu\)</span>, with mean <span class="math inline">\(0\)</span> and covariance <span class="math inline">\(\mathbf{\Omega}\in\mathbb{R}^{k\times k}\)</span>. Thus, the parameter vector <span class="math inline">\(x\in\mathbb{R}^{(N+1)k}\)</span> consists of the <span class="math inline">\(Nk\)</span> elements in the <span class="math inline">\(N\)</span> <span class="math inline">\(\beta_i\)</span> vectors, and the <span class="math inline">\(k\)</span> elements in <span class="math inline">\(\mu\)</span>.</p>
<p>The log posterior density, ignoring any normalization constants, is</p>
<p><span class="math display">\[\begin{aligned}
  \label{eq:LPD}
  \log \pi(\beta_{1:N},\mu|\mathbf{Y}, \mathbf{W}, \mathbf{A},\mathbf{\Omega})=\sum_{i=1}^N\left(p_i^{y_i}(1-p_i)^{T-y_i}
  -\frac{1}{2}\left(\beta_i-\mu\right)^\top\mathbf{A}^{-1}\left(\beta_i-\mu\right)\right)
-\frac{1}{2}\mu^\top\mathbf{\Omega}^{-1}\mu\end{aligned}\]</span></p>
<p>Because one element of <span class="math inline">\(\beta_i\)</span> can be correlated with another element of <span class="math inline">\(\beta_i\)</span> (for the same unit), we allow for the cross-partials between elements of <span class="math inline">\(\beta_i\)</span> for any <span class="math inline">\(i\)</span> to be nonzero. Also, because the mean of each <span class="math inline">\(\beta_i\)</span> depends on <span class="math inline">\(\mu\)</span>, the cross-partials between <span class="math inline">\(\mu\)</span> and any <span class="math inline">\(\beta_i\)</span> can be nonzero. However, since the <span class="math inline">\(\beta_i\)</span> and <span class="math inline">\(\beta_j\)</span> are independent samples, and the <span class="math inline">\(y_i\)</span> are conditionally independent, the cross-partial derivatives between an element of <span class="math inline">\(\beta_i\)</span> and any element of any <span class="math inline">\(\beta_j\)</span> for <span class="math inline">\(j\neq i\)</span>, must be zero. When <span class="math inline">\(N\)</span> is much greater than <span class="math inline">\(k\)</span>, there will be many more zero cross-partial derivatives than nonzero, and the Hessian of the log posterior density will be sparse.</p>
<p>The sparsity pattern depends on how the variables are ordered. One such ordering is to group all of the coefficients in the <span class="math inline">\(\beta_i\)</span> for each unit together, and place <span class="math inline">\(\mu\)</span> at the end.</p>
<p><span class="math display">\[\begin{aligned}
\beta_{11},\dotsc,\beta_{1k},\beta_{21},\dotsc,\beta_{2k},~\dotsc~,\beta_{N1},\dotsc,\beta_{Nk},\mu_1,\dotsc,\mu_k\end{aligned}\]</span></p>
<p>In this case, the Hessian has a “block-arrow” pattern. Figure [[fig(#fig:blockarrow){reference-type=“ref” reference=“fig:blockarrow”} illustrates this pattern for <span class="math inline">\(N=5\)</span> and <span class="math inline">\(k=2\)</span> (12 total variables).</p>
<p>Another possibility is to group coefficients for each covariate together.</p>
<p><span class="math display">\[\begin{aligned} \beta_{11},\dotsc,\beta_{N1},\beta_{12},\dotsc,\beta_{N2},~\dotsc~,\beta_{1k},\dotsc,\beta_{Nk},\mu_1,\dotsc,\mu_k\end{aligned}\]</span></p>
<p>Now the Hessian has an "banded" sparsity pattern, as in Figure (#fig:banded).</p>
<p><img src="block_arrow.png" style="width:40.0%" alt="A block-arrow sparsity pattern"><img src="banded.png" style="width:40.0%" alt="A banded sparsity pattern"></p>
<p>In both cases, the number of nonzeros is the same. There are 144 elements in this symmetric matrix. If the matrix is stored in the standard dense format, memory is reserved for all 144 values, even though only 64 values are nonzero, and only 38 values are unique. For larger matrices, the reduction in memory requirements by storing the matrix in a sparse format can be substantial.[^3]. If <span class="math inline">\(N=\)</span><!-- -->1,000, then <span class="math inline">\(M=\)</span><!-- -->2,002, with more than <span class="math inline">\(4\)</span> million elements in the Hessian. However, only 12,004 of those elements are nonzero, with 7,003 unique values in the lower triangle. The dense matrix requires 30.6 Mb of RAM, while a sparse symmetric matrix of the <em>dsCMatrix</em> class requires only 91.6 Kb.</p>
<p>This example is relevant because, when evaluated at the posterior mode, the Hessian matrix of the log posterior is the MVN precision matrix <span class="math inline">\(\Sigma^{-1}\)</span> of a MVN approximatation to the posterior distribution of <span class="math inline">\(\left(\beta_{1:N},\mu\right)\)</span>. If we were to simulate from this MVN using , or evaluate MVN densities using , we would need to invert <span class="math inline">\(\Sigma^{-1}\)</span> to <span class="math inline">\(\Sigma\)</span> first, and store it as a dense matrix. Internally, and invoke dense linear algebra routines, including matrix factorization.</p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by <a href="https://braunm.github.io" class="external-link external-link">Michael Braun</a>.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link external-link">pkgdown</a> 1.6.1.9001.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
