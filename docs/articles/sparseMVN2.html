<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>The sparseMVN package • sparseMVN</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/spacelab/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="The sparseMVN package">
<meta property="og:description" content="sparseMVN">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-9Q2M3WL4Z1"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9Q2M3WL4Z1');
</script>
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">sparseMVN</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.2.2</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/sparseMVN2.html">The sparseMVN package</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/braunm/sparseMVN/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="sparseMVN2_files/header-attrs-2.11/header-attrs.js"></script><script src="sparseMVN2_files/kePrint-0.0.1/kePrint.js"></script><link href="sparseMVN2_files/lightable-0.0.1/lightable.css" rel="stylesheet">
<div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>The sparseMVN package</h1>
                        <h4 data-toc-skip class="author">Michael Braun, Cox School of Business, Southern Methodist University</h4>
            
            <h4 data-toc-skip class="date">October 26, 2021</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/braunm/sparseMVN/blob/master/vignettes/sparseMVN2.Rmd" class="external-link"><code>vignettes/sparseMVN2.Rmd</code></a></small>
      <div class="hidden name"><code>sparseMVN2.Rmd</code></div>

    </div>

    
    
<p>The <a href="https://braunm.github.io/sparseMVN/">sparseMVN</a> package consists of two user-facing functions: <a href="http://braunm.github.io/sparseMVN/reference/dmvn.sparse.html" class="external-link">dmvn.sparse()</a> computes the density of a multivariate normal (MVN) distribution, and <a href="http://braunm.github.io/sparseMVN/reference/rmvn.sparse.html" class="external-link">rmvn.sparse()</a> samples from an MVN distribution. Unlike MVN functions provided by the <a href="https://cran.r-project.org/package=mvtnorm" class="external-link">mvtnorm</a> and [MASS] packages, which take a dense representation of a full covariance matrix as one of the arguments, the <a href="https://braunm.github.io/sparseMVN/">sparseMVN</a> functions ask for a <strong>sparse</strong> Cholesky decomposition of <strong>either</strong> the covariance or precision matrix. When the dimension of the MVN random variable is large, and the true covariance is itself sparse (i.e., the proportion of nonzero elements is small), <a href="https://braunm.github.io/sparseMVN/">sparseMVN</a> offers the following advantages.</p>
<ol style="list-style-type: decimal">
<li><p>Memory use and computation time in matrix operations: A dense matrix in R stores all <span class="math inline">\(M^2\)</span> elements, even if the matrix is symmetric, and even when a small proportion of elements are nonzero. Thus, memory requirements grow quadratically with the size of the problem. Further, dense Cholesky factorization is a <span class="math inline">\(\mathcal{O}\!\left(M^3\right)\)</span> operation, while multiplication of a triangular matrix with a dense matrix, and solving dense triangular systems, are <span class="math inline">\(\mathcal{O}\!\left(M^2\right)\)</span> <span class="citation">(Golub and Van Loan 1996)</span>. <a href="https://braunm.github.io/sparseMVN/">sparseMVN</a> replies on classes from the <a href="https://CRAN.R-project.org/package=Matrix" class="external-link">Matrix</a> package that compresses sparse matrices into an efficient structure for storage, and provides optimized algorithms for operations on those matrices. So, working with MVN distributions becomes more scalable when the covariance or precision matrix is actually sparse. The reasons are that only nonzero elements are stored explicitly, and redundant multiplications with zero are avoided.</p></li>
<li><p>Avoiding explicit inversion of a precision matrix. In many applications (e.g., sampling from a Laplace approximation), the user starts with a sparse precision matrix rather than a covariance. Using extant MVN functions in R requires the user to explicitly invert the precision matrix, which itself can be an expensive step, with no guarantee of sparsity afterward. But this inversion step is not mathematically necessary; computation of MVN densities and random variates can be done just as easily and directly with the precision matrix. At the user’s option, <a href="https://braunm.github.io/sparseMVN/">sparseMVN</a> accepts a factor of either a sparse covariance or precision matrix.</p></li>
<li><p>Multiple-use Cholesky factorization. MVN computation involves computing the Cholesky factor of the matrix (either covariance or precision). Most MVN functions do this internally (so the user does not have to worry about it) every time the function is called (which is wasteful if the matrix does not change from call to call). <a href="https://braunm.github.io/sparseMVN/">sparseMVN</a> requires the user to perform this step separately with <code><a href="https://rdrr.io/pkg/Matrix/man/Cholesky.html" class="external-link">Matrix::Cholesky()</a></code>. While this feature does impose one additional step on the user, it also allows the user to reuse the same factor when possible.</p></li>
</ol>
<div id="background" class="section level1">
<h1 class="hasAnchor">
<a href="#background" class="anchor" aria-hidden="true"></a>Background</h1>
<p>Let <span class="math inline">\(x\in\mathbb{R}^{M}\)</span> be a realization of random variable <span class="math inline">\(X\sim\mathbf{MVN}\!\left(\mu,\Sigma\right)\)</span>, where <span class="math inline">\(\mu\in\mathbb{R}^{M}\)</span> is a vector, <span class="math inline">\(\Sigma\in\mathbb{R}^{M\times M}\)</span> is a positive-definite covariance matrix, and <span class="math inline">\(\Sigma^{-1}\in\mathbb{R}^{M\times M}\)</span> is a positive-definite precision matrix.</p>
<p>The log probability density of <span class="math inline">\(x\)</span> is</p>
<p><span class="math display">\[\begin{aligned}
\log f(x)&amp;=-\frac{1}{2}\left(M \log (2\pi) + \log|\Sigma|
  +z^\top z\right),\quad\text{where}~z^\top z=\left(x-\mu\right)^\top\Sigma^{-1}\left(x-\mu\right)
 \end{aligned}\]</span></p>
<div id="sec:algorithms" class="section level2">
<h2 class="hasAnchor">
<a href="#sec:algorithms" class="anchor" aria-hidden="true"></a>MVN density computation and random number generation</h2>
<p>The two computationally intensive steps in evaluating <span class="math inline">\(\log f(x)\)</span> are computing <span class="math inline">\(\log|\Sigma|\)</span>, and <span class="math inline">\(z^\top z\)</span>, <em>without</em> explicitly inverting <span class="math inline">\(\Sigma\)</span> or repeating mathematical operations. But once we have <span class="math inline">\(z\)</span>, computing the dot product <span class="math inline">\(z^\top z\)</span> is cheap. How to perform these steps <em>efficiently</em> in practice depends on whether the covariance matrix <span class="math inline">\(\Sigma\)</span>, or the precision matrix <span class="math inline">\(\Sigma^{-1}\)</span> is available. For both cases, we start by finding a lower triangular matrix root: <span class="math inline">\(\Sigma=LL^\top\)</span> or <span class="math inline">\(\Sigma^{-1}=\Lambda\Lambda^\top\)</span>. Since <span class="math inline">\(\Sigma\)</span> and <span class="math inline">\(\Sigma^{-1}\)</span> are positive definite, we will use the Cholesky decomposition, which is the unique matrix root with all positive elements on the diagonal.</p>
<p>With the Cholesky decomposition in hand, we compute the log determinant of <span class="math inline">\(\Sigma\)</span> by adding the logs of the diagonal elements of the factors. <span class="math display">\[\begin{aligned}
  \label{eq:logDet}
  \log|\Sigma|= \begin{cases}
    \phantom{-}2\sum_{m=1}^M\log L_{mm}&amp;\text{ when $\Sigma$ is given}\\
    -2\sum_{m=1}^M\log \Lambda_{mm}&amp;\text{ when $\Sigma^{-1}$ is given}
    \end{cases}\end{aligned}\]</span></p>
<p>Having already computed the triangular matrix roots also speeds up the computation of <span class="math inline">\(z^\top z\)</span>. If <span class="math inline">\(\Sigma^{-1}\)</span> is given, <span class="math inline">\(z=\Lambda^\top(x-\mu)\)</span> can be computed efficiently as the product of an upper triangular matrix and a vector. When <span class="math inline">\(\Sigma\)</span> is given, we find <span class="math inline">\(z\)</span> by solving the lower triangular system <span class="math inline">\(Lz=x-\mu\)</span>. The subsequent <span class="math inline">\(z^\top z\)</span> computation is trivially fast.</p>
<p>The algorithm for simulating <span class="math inline">\(X\sim\mathbf{MVN}\!\left(\mu,\Sigma\right)\)</span> also depends on whether <span class="math inline">\(\Sigma\)</span> or <span class="math inline">\(\Sigma^{-1}\)</span> is given. As above, we start by computing the Cholesky decomposition of the given covariance or precision matrix. Define a random variable <span class="math inline">\(Z\sim\mathbf{MVN}\!\left(0,I_M\right)\)</span>, and generate a realization <span class="math inline">\(z\)</span> as a vector of <span class="math inline">\(M\)</span> samples from a standard normal distribution. If <span class="math inline">\(\Sigma\)</span> is given, then evaluate <span class="math inline">\(x=Lz+\mu\)</span>. If <span class="math inline">\(\Sigma^{-1}\)</span> is given, then solve for <span class="math inline">\(x\)</span> in the triangular linear system <span class="math inline">\(\Lambda^\top\left(x-\mu\right)=z\)</span>. The resulting <span class="math inline">\(x\)</span> is a sample from <span class="math inline">\(\mathbf{MVN}\!\left(\mu,\Sigma\right)\)</span>. The following shows that this approach is correct:</p>
<p><span class="math display">\[\begin{aligned}
 \mathbf{E}\!\left(X\right)&amp;=\mathbf{E}\!\left(LZ+\mu\right)=\mathbf{E}\!\left(\Lambda^\top Z+\mu\right)=\mu\\
   \mathbf{cov}\!\left(X\right)&amp;= \mathbf{cov}\!\left(LZ+\mu\right)=\mathbf{E}\!\left(LZZ^\top L^\top\right)=LL^\top=\Sigma\\
     \mathbf{cov}\!\left(X\right)&amp;=\mathbf{cov}\!\left((\Lambda^\top)^{-1}Z+\mu\right)=\mathbf{E}\!\left((\Lambda^\top)^{-1}ZZ^\top\Lambda^{-1}\right)
     =(\Lambda^\top)^{-1}\Lambda^{-1}=(\Lambda\Lambda^\top)^{-1}=\Sigma
 \end{aligned}\]</span></p>
<p>These algorithms apply when the covariance/precision matrix is either sparse or dense. When the matrix is dense, the computational complexity is <span class="math inline">\(\mathcal{O}\!\left(M^3\right)\)</span> for a Cholesky decomposition, and <span class="math inline">\(\mathcal{O}\!\left(M^2\right)\)</span> for either solving the triangular linear system or multiplying a triangular matrix by another matrix <span class="citation">(Golub and Van Loan 1996)</span>. Thus, the computational cost grows cubically with <span class="math inline">\(M\)</span> before the decomposition step, and quadratically if the decomposition has already been completed. Additionally, the storage requirement for <span class="math inline">\(\Sigma\)</span> (or <span class="math inline">\(\Sigma^{-1}\)</span>) grows quadratically with <span class="math inline">\(M\)</span>.</p>
</div>
<div id="sec:sparse" class="section level2">
<h2 class="hasAnchor">
<a href="#sec:sparse" class="anchor" aria-hidden="true"></a>Sparse matrices in R</h2>
<p>The <a href="https://CRAN.R-project.org/package=Matrix" class="external-link">Matrix</a> package <span class="citation">(Bates and Maechler 2017)</span> defines various classes for storing sparse matrices in compressed formats. The most important class for our purposes is <a href="https://rdrr.io/cran/Matrix/man/dsCMatrix-class.html" class="external-link">dsCMatrix</a>, which defines a symmetric matrix, with numeric (double precision) elements, in a column-compressed format. Three vectors define the underlying matrix: the unique nonzero values (just one triangle is needed), the indices in the value vector for the first value in each column, and the indices of the rows in which each value is located. The storage requirements for a sparse <span class="math inline">\(M\times M\)</span> symmetric matrix with <span class="math inline">\(V\)</span> unique nonzero elements in one triangle are for <span class="math inline">\(V\)</span> double precision numbers, <span class="math inline">\(V+M+1\)</span> integers, and some metadata. In contrast, a dense representation of the same matrix stores <span class="math inline">\(M^2\)</span> double precision values, regardless of symmetry and the number of zeros. If <span class="math inline">\(V\)</span> grows more slowly than <span class="math inline">\(M^2\)</span>, the matrix becomes increasingly sparse (a smaller percentage of elements are nonzero), with greater efficiency gains from storing the matrix in a compressed sparse format.</p>
<p>To illustrate how sparse matrices require less memory resources when compressed than when stored densely, consider the following example, which borrows heavily from the vignette of the <a href="https://braunm.github.io/sparseHessianFD/" class="external-link">sparseHessianFD</a> package.</p>
<p>Suppose we have a dataset of <span class="math inline">\(N\)</span> households, each with <span class="math inline">\(T\)</span> opportunities to purchase a particular product. Let <span class="math inline">\(y_i\)</span> be the number of times household <span class="math inline">\(i\)</span> purchases the product, out of the <span class="math inline">\(T\)</span> purchase opportunities, and let <span class="math inline">\(p_i\)</span> be the probability of purchase. The heterogeneous parameter <span class="math inline">\(p_i\)</span> is the same for all <span class="math inline">\(T\)</span> opportunities, so <span class="math inline">\(y_i\)</span> is a binomial random variable.</p>
<p>Let <span class="math inline">\(\beta_i\in\mathbb{R}^{k}\)</span> be a heterogeneous coefficient vector that is specific to household <span class="math inline">\(i\)</span>, such that <span class="math inline">\(\beta_i=(\beta_{i1},\dotsc,\beta_{ik})\)</span>. Similarly, <span class="math inline">\(w_i\in\mathbb{R}^{k}\)</span> is a vector of household-specific covariates. Define each <span class="math inline">\(p_i\)</span> such that the log odds of <span class="math inline">\(p_i\)</span> is a linear function of <span class="math inline">\(\beta_i\)</span> and <span class="math inline">\(w_i\)</span>, but does not depend directly on <span class="math inline">\(\beta_j\)</span> and <span class="math inline">\(w_j\)</span> for another household <span class="math inline">\(j\neq i\)</span>: <span class="math display">\[\begin{aligned}
  p_i=\frac{\exp(w_i'\beta_i)}{1+\exp(w_i'\beta_i)},~i=1,\dots, N\end{aligned}\]</span></p>
<p>The coefficient vectors <span class="math inline">\(\beta_i\)</span> are distributed across the population of households following a MVN distribution with mean <span class="math inline">\(\mu\in\mathbb{R}^{k}\)</span> and covariance <span class="math inline">\(\mathbf{A}\in\mathbb{R}^{k\times k}\)</span>. Assume that we know <span class="math inline">\(\mathbf{A}\)</span>, but not <span class="math inline">\(\mu\)</span>, so we place a multivariate normal prior on <span class="math inline">\(\mu\)</span>, with mean <span class="math inline">\(0\)</span> and covariance <span class="math inline">\(\mathbf{\Omega}\in\mathbb{R}^{k\times k}\)</span>. Thus, the parameter vector <span class="math inline">\(x\in\mathbb{R}^{(N+1)k}\)</span> consists of the <span class="math inline">\(Nk\)</span> elements in the <span class="math inline">\(N\)</span> <span class="math inline">\(\beta_i\)</span> vectors, and the <span class="math inline">\(k\)</span> elements in <span class="math inline">\(\mu\)</span>.</p>
<p>The log posterior density, ignoring any normalization constants, is <span class="math display">\[\begin{aligned}
  \label{eq:LPD}
  \log \pi(\beta_{1:N},\mu|\mathbf{Y}, \mathbf{W}, \mathbf{A},\mathbf{\Omega})=\sum_{i=1}^N\left(p_i^{y_i}(1-p_i)^{T-y_i}
  -\frac{1}{2}\left(\beta_i-\mu\right)^\top\mathbf{A}^{-1}\left(\beta_i-\mu\right)\right)
-\frac{1}{2}\mu^\top\mathbf{\Omega}^{-1}\mu\end{aligned}\]</span></p>
<p>Because one element of <span class="math inline">\(\beta_i\)</span> can be correlated with another element of <span class="math inline">\(\beta_i\)</span> (for the same unit), we allow for the cross-partials between elements of <span class="math inline">\(\beta_i\)</span> for any <span class="math inline">\(i\)</span> to be nonzero. Also, because the mean of each <span class="math inline">\(\beta_i\)</span> depends on <span class="math inline">\(\mu\)</span>, the cross-partials between <span class="math inline">\(\mu\)</span> and any <span class="math inline">\(\beta_i\)</span> can be nonzero. However, since the uild<span class="math inline">\(\beta_i\)</span> and <span class="math inline">\(\beta_j\)</span> are independent samples, and the <span class="math inline">\(y_i\)</span> are conditionally independent, the cross-partial derivatives between an element of <span class="math inline">\(\beta_i\)</span> and any element of any <span class="math inline">\(\beta_j\)</span> for <span class="math inline">\(j\neq i\)</span>, must be zero. When <span class="math inline">\(N\)</span> is much greater than <span class="math inline">\(k\)</span>, there will be many more zero cross-partial derivatives than nonzero, and the Hessian of the log posterior density will be sparse.</p>
<p>The sparsity pattern depends on how the variables are ordered. One such ordering is to group all of the coefficients in the <span class="math inline">\(\beta_i\)</span> for each unit together, and place <span class="math inline">\(\mu\)</span> at the end. <span class="math display">\[\begin{aligned} \beta_{11},\dotsc,\beta_{1k},\beta_{21},\dotsc,\beta_{2k},~\dotsc~,\beta_{N1},\dotsc,\beta_{Nk},\mu_1,\dotsc,\mu_k\end{aligned}\]</span> In this case, the Hessian has a “block-arrow” pattern. This figure illustrates this pattern for <span class="math inline">\(N=5\)</span> and <span class="math inline">\(k=2\)</span> (12 total variables).</p>
<div class="figure" style="text-align: center">
<img src="block_arrow.png" alt="Block arrow (left) and banded (right) sparsity patterns" width="42%" height="42%"><img src="banded.png" alt="Block arrow (left) and banded (right) sparsity patterns" width="42%" height="42%"><p class="caption">
Block arrow (left) and banded (right) sparsity patterns
</p>
</div>
<!-- ```{r blockarrow, echo=TRUE,results="asis"} -->
<!-- Mat1a <- as(kronecker(diag(N), matrix(1, k, k)),"sparseMatrix") -->
<!-- Mat1a <- rbind(Mat1a, Matrix(1, p, N*k)) -->
<!-- Mat1a <- cbind(Mat1a, Matrix(1, k*N+p, p)) -->
<!-- tmp_a <- as.matrix(Mat1a) -->
<!-- colnames(tmp_a) <- letters[1:NCOL(tmp_a)] -->
<!-- tmp_a %>% -->
<!--   as_tibble(.name_repair="minimal") %>% -->
<!--   mutate(across(colnames(tmp_a), ~ifelse(.x == 1, "|", "."))) %>% -->
<!--   kbl(col.names=NULL, caption="A block-arrow pattern") ## %>% -->
<!--   ## kable_minimal() -->
<!-- ``` -->
<p>Another possibility is to group coefficients for each covariate together. <span class="math display">\[\begin{aligned} \beta_{11},\dotsc,\beta_{N1},\beta_{12},\dotsc,\beta_{N2},~\dotsc~,\beta_{1k},\dotsc,\beta_{Nk},\mu_1,\dotsc,\mu_k\end{aligned}\]</span> Now the Hessian has an "banded" sparsity pattern:</p>
<!-- ```{r, banded, echo=FALSE,results="asis"} -->
<!-- Mat1b <- kronecker(Matrix(1, k, k), diag(N)) -->
<!-- Mat1b <- rbind(Mat1b, Matrix(1, p, N * k)) -->
<!-- Mat1b <- cbind(Mat1b, Matrix(1, k*N+p, p)) -->
<!-- tmp_b <- as.matrix(Mat1b) -->
<!-- colnames(tmp_b) <- letters[1:NCOL(tmp_b)] -->
<!-- tmp_b  %>% -->
<!--   as_tibble(.name_repair="minimal") %>% -->
<!--   mutate(across(colnames(tmp_b), ~ifelse(.x == 1, "|", "."))) %>% -->
<!--   kbl(col.names=NULL, caption="A banded sparsity pattern") %>% -->
<!--   kable_minimal() -->
<!-- ``` -->
<p>In both cases, the number of nonzeros is the same. There are 144 elements in this symmetric matrix. If the matrix is stored in the standard base R dense format, memory is reserved for all 144 values, even though only 64 values are nonzero, and only 38 values are unique. For larger matrices, the reduction in memory requirements by storing the matrix in a sparse format can be substantial. If <span class="math inline">\(N=1,000\)</span> and <span class="math inline">\(k=2\)</span>, then <span class="math inline">\(M=\)</span> 2,002, with more than <span class="math inline">\(4\)</span> million elements in the Hessian. However, only 12,004 of those elements are nonzero, with 7,003 unique values in the lower triangle. The dense matrix requires 30.6 Mb of RAM, while a sparse symmetric <a href="https://rdrr.io/cran/Matrix/man/dsCMatrix-class.html" class="external-link">dsCMatrix</a> matrix requires only 91.6 Kb.</p>
<p>This example is relevant because, when evaluated at the posterior mode, the Hessian matrix of the log posterior is the MVN precision matrix <span class="math inline">\(\Sigma^{-1}\)</span> of a MVN approximation to the posterior distribution of <span class="math inline">\(\left(\beta_{1:N},\mu\right)\)</span>. If we were to simulate from this MVN using , or evaluate MVN densities using , we would need to invert <span class="math inline">\(\Sigma^{-1}\)</span> to <span class="math inline">\(\Sigma\)</span> first, and store it as a dense matrix. Internally, and invoke dense linear algebra routines, including matrix factorization.</p>
</div>
</div>
<div id="using-the-sparsemvn-package" class="section level1">
<h1 class="hasAnchor">
<a href="#using-the-sparsemvn-package" class="anchor" aria-hidden="true"></a>Using the sparseMVN package</h1>
<p>The signatures of the key <a href="https://braunm.github.io/sparseMVN/">sparseMVN</a> functions are</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="../reference/rmvn.sparse.html">rmvn.sparse</a></span><span class="op">(</span><span class="va">n</span>, <span class="va">mu</span>, <span class="va">CH</span>, prec<span class="op">=</span><span class="cn">TRUE</span>, log<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span>
<span class="fu"><a href="../reference/dmvn.sparse.html">dmvn.sparse</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">mu</span>, <span class="va">CH</span>, prec<span class="op">=</span><span class="cn">TRUE</span>, log<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<p><a href="http://braunm.github.io/sparseMVN/reference/rmvn.sparse.html" class="external-link">rmvn.sparse()</a> returns a matrix <span class="math inline">\(x\)</span> with <span class="math inline">\(n\)</span> rows and <code>length(mu)</code> columns. <a href="http://braunm.github.io/sparseMVN/reference/dmvn.sparse.html" class="external-link">dmvn.sparse()</a> returns a vector of length n: densities if <code>log=FALSE</code> , and log densities if <code>log=TRUE</code>.</p>
<table class="table">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<tbody>
<tr class="odd">
<td><strong>x</strong></td>
<td>A numeric matrix. Each row is an MVN sample.</td>
</tr>
<tr class="even">
<td><strong>mu</strong></td>
<td>A numeric vector. The mean of the MVN random variable.</td>
</tr>
<tr class="odd">
<td><strong>CH</strong></td>
<td>Either a <a href="https://rdrr.io/cran/Matrix/man/CHMfactor-class.html" class="external-link">dCHMsimpl</a> or <a href="https://rdrr.io/cran/Matrix/man/CHMfactor-class.html" class="external-link">dCHMsuper</a> object representing the Cholesky decomposition of the covariance/precision matrix.</td>
</tr>
<tr class="even">
<td><strong>prec</strong></td>
<td>Logical value that identifies CH as the Cholesky decomposition of either a covariance (<span class="math inline">\(\Sigma\)</span>, ) or precision(<span class="math inline">\(\Sigma^{-1}\)</span>, ) matrix.</td>
</tr>
<tr class="odd">
<td><strong>n</strong></td>
<td>Number of random samples to be generated.</td>
</tr>
<tr class="even">
<td><strong>log</strong></td>
<td>If <code>log=TRUE</code>, the log density is returned.</td>
</tr>
</tbody>
</table>
<p>[Table 1: Arguments to <a href="http://braunm.github.io/sparseMVN/reference/dmvn.sparse.html" class="external-link">dmvn.sparse()</a> and <a href="http://braunm.github.io/sparseMVN/reference/rmvn.sparse.html" class="external-link">rmvn.sparse()</a>]</p>
<p><a href="http://braunm.github.io/sparseMVN/reference/dmvn.sparse.html" class="external-link">dmvn.sparse()</a> and <a href="http://braunm.github.io/sparseMVN/reference/rmvn.sparse.html" class="external-link">rmvn.sparse()</a> require the user to compute the Cholesky decomposition <code>CH</code> beforehand, but this needs to be done only once (as long as <span class="math inline">\(\Sigma\)</span> or <span class="math inline">\(\Sigma^{-1}\)</span> does not change). <code>CH</code> should be computed using <a href="https://rdrr.io/cran/Matrix/man/Cholesky.html" class="external-link">Cholesky()</a>, whose first argument is a sparse symmetric matrix stored as a <a href="https://rdrr.io/cran/Matrix/man/dsCMatrix-class.html" class="external-link">dsCMatrix</a> object. As far as we know, there is no particular need to deviate from the defaults of the remaining arguments. If <a href="https://rdrr.io/cran/Matrix/man/Cholesky.html" class="external-link">Cholesky()</a> uses a fill-reducing permutation to compute <code>CH</code> , the sparseMVN functions will handle that directly, with no additional user intervention required. The <a href="https://rdrr.io/r/base/chol.html" class="external-link">chol()</a> function in base R should not be used.</p>
<div id="sec:example" class="section level2">
<h2 class="hasAnchor">
<a href="#sec:example" class="anchor" aria-hidden="true"></a>An example</h2>
<p>Suppose we want to generate samples from an MVN approximation to the posterior distribution of our example model from Section <a href="#sec:sparse" reference-type="ref" reference="sec:sparse">1.2</a>. <a href="https://braunm.github.io/sparseMVN/">sparseMVN</a> includes <a href="http://braunm.github.io/sparseMVN/reference/binary.html" class="external-link">functions</a> to simulate data and compute the log posterior density, gradient and Hessian for the binary choice example model. The <a href="https://braunm.github.io/trustOptim/" class="external-link">trustOptim</a> package provides a nonlinear optimizer that estimates the curvature of the objective function using a sparse Hessian.</p>
<p>The calls to <a href="https://braunm.github.io/trustOptim/reference/trust.optim.html" class="external-link">trust.optim()</a> return the posterior mode, and the Hessian evaluated at the mode. These results serve as the mean and the negative precision of the MVN approximation to the posterior distribution of the model.</p>
<p>We can then sample from the posterior using an MVN approximation, and compute the MVN log density for each sample.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">samples</span> <span class="op">&lt;-</span> <span class="fu">sparseMVN</span><span class="fu">::</span><span class="fu"><a href="../reference/rmvn.sparse.html">rmvn.sparse</a></span><span class="op">(</span><span class="va">R</span>, <span class="va">pm</span>, <span class="va">CH</span>, prec<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span>
<span class="va">logf</span> <span class="op">&lt;-</span> <span class="fu">sparseMVN</span><span class="fu">::</span><span class="fu"><a href="../reference/dmvn.sparse.html">dmvn.sparse</a></span><span class="op">(</span><span class="va">samples</span>, <span class="va">pm</span>, <span class="va">CH</span>, prec<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<p>The ability to accept a precision matrix, rather than having to invert it to a covariance matrix, is a valuable feature of <a href="https://braunm.github.io/sparseMVN/">sparseMVN</a>. This is because the inverse of a sparse matrix is not necessarily sparse. In the following chunk, we invert the Hessian, and drop zero values to maintain any remaining sparseness. Note that there are 10,404 total elements in the Hessian.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">Matrix</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/nnzero.html" class="external-link">nnzero</a></span><span class="op">(</span><span class="va">H</span><span class="op">)</span>
<span class="va">Hinv</span> <span class="op">&lt;-</span> <span class="fu">Matrix</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/drop0.html" class="external-link">drop0</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html" class="external-link">solve</a></span><span class="op">(</span><span class="va">H</span><span class="op">)</span><span class="op">)</span>
<span class="fu">Matrix</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/nnzero.html" class="external-link">nnzero</a></span><span class="op">(</span><span class="va">Hinv</span><span class="op">)</span></code></pre></div>
<p>Nevertheless, we should check that the log densities from <a href="http://braunm.github.io/sparseMVN/reference/dmvn.sparse.html" class="external-link">dmvn.sparse()</a> correspond to those that we would get from <a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html" class="external-link">dmvnorm()</a>.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">logf_dense</span> <span class="op">&lt;-</span> <span class="fu">mvtnorm</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html" class="external-link">dmvnorm</a></span><span class="op">(</span><span class="va">samples</span>, <span class="va">pm</span>, <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="va">Hinv</span><span class="op">)</span>, log<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/all.equal.html" class="external-link">all.equal</a></span><span class="op">(</span><span class="va">logf</span>, <span class="va">logf_dense</span><span class="op">)</span></code></pre></div>
</div>
</div>
<div id="performance" class="section level1">
<h1 class="hasAnchor">
<a href="#performance" class="anchor" aria-hidden="true"></a>Performance</h1>
<p>In this section we show the efficiency gains from sparseMVN by comparing the run times between <a href="http://braunm.github.io/sparseMVN/reference/rmvn.sparse.html" class="external-link">rmvn.sparse()</a> and <a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html" class="external-link">rmvnorm()</a>, and between <a href="http://braunm.github.io/sparseMVN/reference/dmvn.sparse.html" class="external-link">dmvn.sparse()</a> and <code><a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html" class="external-link">mvtnorm::dmvnorm</a></code>. In these tests, we construct covariance/precision matrices with the same structure as the Hessian of the log posterior density of the example model in Section <a href="#sec:example" reference-type="ref" reference="sec:example">2.1</a>. Parameters are ordered such that the matrix has a block-arrow pattern (Figure @ref(fig:sparse-patterns), left).</p>
<p>The following table summarizes the case conditions. <span class="math inline">\(N\)</span> and <span class="math inline">\(k\)</span> refer, respectively, to the number of blocks in the block-arrow structure (analogous to heterogeneous units in the binary choice example), and the size of each block. The total number of variables is <span class="math inline">\(M=(N+1)k\)</span>, and the total number of elements in the matrix is <span class="math inline">\(M^2\)</span>. The three rightmost columns present the number of nonzeros in the full matrix and lower triangle, and the sparsity (proportion of matrix elements that are nonzero. The size and sparsity of the test matrices vary through manipulation of the number of blocks (<span class="math inline">\(N\)</span>), the size of each block (<span class="math inline">\(k\)</span>), and the number of rows/columns in the margin (also <span class="math inline">\(k\)</span>). Each test matrix has <span class="math inline">\((N+1)k\)</span> rows and columns.</p>
<table class="table  lightable-minimal" style='font-family: "Trebuchet MS", verdana, sans-serif; margin-left: auto; margin-right: auto;'>
<caption>
Cases for the timing comparison.
</caption>
<thead>
<tr>
<th style="empty-cells: hide;" colspan="4">
</th>
<th style="padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 2px solid #00000050; ">
nonzeros
</div>
</th>
</tr>
<tr>
<th style="text-align:right;">
k
</th>
<th style="text-align:right;">
N
</th>
<th style="text-align:right;">
variables
</th>
<th style="text-align:right;">
elements
</th>
<th style="text-align:right;">
full
</th>
<th style="text-align:right;">
lower tri
</th>
<th style="text-align:right;">
sparsity
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
484
</td>
<td style="text-align:right;">
124
</td>
<td style="text-align:right;">
73
</td>
<td style="text-align:right;">
0.256
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
42
</td>
<td style="text-align:right;">
1,764
</td>
<td style="text-align:right;">
244
</td>
<td style="text-align:right;">
143
</td>
<td style="text-align:right;">
0.138
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
102
</td>
<td style="text-align:right;">
10,404
</td>
<td style="text-align:right;">
604
</td>
<td style="text-align:right;">
353
</td>
<td style="text-align:right;">
0.058
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
100
</td>
<td style="text-align:right;">
202
</td>
<td style="text-align:right;">
40,804
</td>
<td style="text-align:right;">
1,204
</td>
<td style="text-align:right;">
703
</td>
<td style="text-align:right;">
0.030
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
200
</td>
<td style="text-align:right;">
402
</td>
<td style="text-align:right;">
161,604
</td>
<td style="text-align:right;">
2,404
</td>
<td style="text-align:right;">
1,403
</td>
<td style="text-align:right;">
0.015
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
300
</td>
<td style="text-align:right;">
602
</td>
<td style="text-align:right;">
362,404
</td>
<td style="text-align:right;">
3,604
</td>
<td style="text-align:right;">
2,103
</td>
<td style="text-align:right;">
0.010
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
400
</td>
<td style="text-align:right;">
802
</td>
<td style="text-align:right;">
643,204
</td>
<td style="text-align:right;">
4,804
</td>
<td style="text-align:right;">
2,803
</td>
<td style="text-align:right;">
0.007
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
500
</td>
<td style="text-align:right;">
1,002
</td>
<td style="text-align:right;">
1,004,004
</td>
<td style="text-align:right;">
6,004
</td>
<td style="text-align:right;">
3,503
</td>
<td style="text-align:right;">
0.006
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
44
</td>
<td style="text-align:right;">
1,936
</td>
<td style="text-align:right;">
496
</td>
<td style="text-align:right;">
270
</td>
<td style="text-align:right;">
0.256
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
84
</td>
<td style="text-align:right;">
7,056
</td>
<td style="text-align:right;">
976
</td>
<td style="text-align:right;">
530
</td>
<td style="text-align:right;">
0.138
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
204
</td>
<td style="text-align:right;">
41,616
</td>
<td style="text-align:right;">
2,416
</td>
<td style="text-align:right;">
1,310
</td>
<td style="text-align:right;">
0.058
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
100
</td>
<td style="text-align:right;">
404
</td>
<td style="text-align:right;">
163,216
</td>
<td style="text-align:right;">
4,816
</td>
<td style="text-align:right;">
2,610
</td>
<td style="text-align:right;">
0.030
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
200
</td>
<td style="text-align:right;">
804
</td>
<td style="text-align:right;">
646,416
</td>
<td style="text-align:right;">
9,616
</td>
<td style="text-align:right;">
5,210
</td>
<td style="text-align:right;">
0.015
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
300
</td>
<td style="text-align:right;">
1,204
</td>
<td style="text-align:right;">
1,449,616
</td>
<td style="text-align:right;">
14,416
</td>
<td style="text-align:right;">
7,810
</td>
<td style="text-align:right;">
0.010
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
400
</td>
<td style="text-align:right;">
1,604
</td>
<td style="text-align:right;">
2,572,816
</td>
<td style="text-align:right;">
19,216
</td>
<td style="text-align:right;">
10,410
</td>
<td style="text-align:right;">
0.007
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
500
</td>
<td style="text-align:right;">
2,004
</td>
<td style="text-align:right;">
4,016,016
</td>
<td style="text-align:right;">
24,016
</td>
<td style="text-align:right;">
13,010
</td>
<td style="text-align:right;">
0.006
</td>
</tr>
</tbody>
</table>
<p>Figure @ref(fig:densRand) compares mean run times to compute 1,000 MVN densities, and generate 1,000 MVN samples, using <a href="http://braunm.github.io/sparseMVN/reference/rmvn.sparse.html" class="external-link">rmvn.sparse()</a> and <a href="http://braunm.github.io/sparseMVN/reference/dmvn.sparse.html" class="external-link">dmvn.sparse()</a> from <a href="https://braunm.github.io/sparseMVN/">sparseMVN</a>, and <a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html" class="external-link">dmvnorm()</a> and <a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html" class="external-link">rmvnorm()</a> in <a href="https://braunm.github.io/sparseMVN/">sparseMVN</a>. Times were collected over 200 replications on a 2013-vintage Mac Pro with a 12-core 2.7 GHz Intel Xeon E5 processor and 64 GB of RAM. The times for <a href="https://cran.r-project.org/package=mvtnorm" class="external-link">mvtnorm</a> are faster than <a href="https://braunm.github.io/sparseMVN/">sparseMVN</a> for low dimensional cases (<span class="math inline">\(N\leq 50\)</span>), but grow quadratically in the number of variables. This is because the number of elements stored in a dense covariance matrix grows quadratically with the number of variables. In this example, storage and computation requirements for the sparse matrix grow linearly with the number of variables, so the <a href="https://braunm.github.io/sparseMVN/">sparseMVN</a> run times grow linearly as well <span class="citation">(Braun and Damien 2016)</span>. The comparative advantage of <a href="https://braunm.github.io/sparseMVN/">sparseMVN</a> increases with the sparsity of the covariance matrix.[^5]</p>
<div class="figure">
<img src="sparseMVN2_files/figure-html/densRand-1.png" alt="Mean computation time for simulating 1,000 MVN samples, and computing 1,000 MVN densities, averaged over 200 replications. Densities were computed using [dmvnorm] and [dmvn.sparse()], while random samples were generated using [rmvnorm()] and [rmvn.sparse()]." width="90%"><p class="caption">
Mean computation time for simulating 1,000 MVN samples, and computing 1,000 MVN densities, averaged over 200 replications. Densities were computed using [dmvnorm] and <a href="http://braunm.github.io/sparseMVN/reference/dmvn.sparse.html" class="external-link">dmvn.sparse()</a>, while random samples were generated using <a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html" class="external-link">rmvnorm()</a> and <a href="http://braunm.github.io/sparseMVN/reference/rmvn.sparse.html" class="external-link">rmvn.sparse()</a>.
</p>
</div>
<p>The <a href="https://braunm.github.io/sparseMVN/">sparseMVN</a> functions always require a sparse Cholesky decomposition of the covariance or precision matrix, and the <a href="https://cran.r-project.org/package=mvtnorm" class="external-link">mvtnorm</a> functions require a dense precision matrix to be inverted into a dense covariance matrix. Figure @ref(fig:cholSolve) compares the computation times of these preparatory steps. There are three cases to consider: inverting a dense matrix using <a href="https://rdrr.io/r/base/solve.html" class="external-link">solve()</a>, decomposing a sparse matrix using Matrix::<a href="https://rdrr.io/cran/Matrix/man/Cholesky.html" class="external-link">Cholesky()</a>, and decomposing a dense matrix using <a href="https://rdrr.io/r/base/chol.html" class="external-link">chol()</a>. Applying <a href="https://rdrr.io/r/base/chol.html" class="external-link">chol()</a> to a dense matrix is not a required operation in advance of calling <a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html" class="external-link">dmvnorm()</a> or <a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html" class="external-link">rmvnorm()</a>, but those functions will invoke some kind of decomposition internally. We include it in our comparison because it comprises a substantial part of the computation time. The decomposition and inversion operations on the dense matrices grow cubically as the size of the matrix increases. The sparse Cholesky decomposition time is negligible. For example, the mean run time for the <span class="math inline">\(N=500\)</span>, <span class="math inline">\(k=4\)</span> case is about 0.39 ms.</p>
<div class="figure">
<img src="sparseMVN2_files/figure-html/cholSolve-1.png" alt="Computation time for Cholesky decomposition of sparse and dense matrices, and inversion of dense matrices." width="90%"><p class="caption">
Computation time for Cholesky decomposition of sparse and dense matrices, and inversion of dense matrices.
</p>
</div>
</div>
<div id="references" class="section level1">
<h1 class="hasAnchor">
<a href="#references" class="anchor" aria-hidden="true"></a>References</h1>
<!-- [^2]: LaplacesDemon does offer options for the user to supply pre-factored covariance and precision matrices. This avoids repeated calls to the <!-- $\mathcal{O}\!\left(M^3\right)$ factorization step, but not the $\mathcal{O}\!\left(M^2\right)$ matrix multiplication and linear system solution steps. -\-> -->
<!-- [^3]: Because sparse matrix structures store row and column indices of -->
<!--     the nonzero values, they may use more memory than dense storage if -->
<!--     the total number of elements is small -->
<!-- [^4]: As an example, in the $N=10$, $k=2$ case, the mean time to compute -->
<!--     1,000 MVN densities is 1.1 ms using , but more than 3.7 ms using . -->
<!-- [^5]: Across all cases there was hardly any difference in the run times -->
<!--     when providing the precision matrix instead of the covariance. -->
<!-- However, for many applications the covariance or precision matrix is sparse, meaning that the proportion of nonzero elements is small, relative to the total size of the matrix. The functions in the [sparseMVN] package exploit that sparsity to reduce memory requirements, and to gain computational efficiencies. The [dmvn.sparse()] function computes the MVN density, and the [rmvn.sparse()] function samples from an MVN distribution. Instead of requiring the user to supply a dense covariance matrix, [rmvn.sparse()] and [dmvn.sparse()] accept a pre-computed Cholesky decomposition of either the covariance or precision matrix in a compressed sparse format. This approach has several advantages: -->
<!-- 1.  Memory requirements are smaller because only the nonzero elements of the matrix are stored in a compressed sparse format. -->
<!-- 2.  Linear algebra algorithms that are optimized for sparse matrices are more efficient because they avoid operations on matrix elements that are known to be zero. -->
<!-- 3.  When the precision matrix is initially available, the user can avoid the need to explicitly invert it into a covariance matrix. This feature of preserves sparsity, because the inverse of a sparse matrix is not necessarily sparse. -->
<!-- 4.  The Cholesky factor of the matrix is computed once, before the first function call, and is not repeated with subsequent calls (as long as the matrix does not change). -->
<!-- The functions in [sparseMVN] rely on sparse matrix classes and functions defined in the [Matrix] package. The user creates the covariance or precision matrix as a sparse, symmetric [dsCMatrix] matrix, and computes the sparse Cholesky factor using the Matrix::Cholesky() function. Other than ensuring that the factor for the covariance or precision matrix is in the correct format, the [sparseMVN] functions behave in much the same way as the corresponding [mvtnorm] functions. Internally, [sparseMVN]  uses standard methods of computing the MVN density and simulating MVN random variables (see Section [1.1](#sec:algorithms){reference-type="ref" reference="sec:algorithms"}). Since a large proportion of elements in the matrix are zero, we need to store only the row and column indices, and the values, of the unique nonzero elements. The efficiency gains in [sparseMVN] come from storing the covariance or precision matrix in a compressed format without explicit zeros, and applying linear algebra routines that are optimized for those sparse matrix structures. [Matrix] calls sparse linear algebra routines that are implemented in the `CHOLMOD` library [@ChenDavis2008; @DavisHager1999; @DavisHager2009]. -->
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-R_Matrix" class="csl-entry">
Bates, Douglas, and Martin Maechler. 2017. <em>: Sparse and Dense Matrix Classes and Methods</em>. <a href="https://CRAN.R-project.org/package=Matrix" class="external-link">https://CRAN.R-project.org/package=Matrix</a>.
</div>
<div id="ref-BraunDamien2016" class="csl-entry">
Braun, Michael, and Paul Damien. 2016. <span>“Scalable Rejection Sampling for <span>B</span>ayesian Hierarchical Models.”</span> <em>Marketing Science</em> 35 (3): 427–44. <a href="https://doi.org/10.1287/mksc.2014.0901" class="external-link">https://doi.org/10.1287/mksc.2014.0901</a>.
</div>
<div id="ref-GolubVanLoan1996" class="csl-entry">
Golub, Gene H, and Charles F Van Loan. 1996. <em>Matrix Computations</em>. 3rd ed. Johns Hopkins University Press.
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by <a href="https://braunm.github.io" class="external-link external-link">Michael Braun</a>.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link external-link">pkgdown</a> 1.6.1.9001.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
