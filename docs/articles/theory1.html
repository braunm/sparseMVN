<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>sparseMVN: The Theory â€¢ sparseMVN</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/spacelab/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="sparseMVN: The Theory">
<meta property="og:description" content="sparseMVN">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-9Q2M3WL4Z1"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9Q2M3WL4Z1');
</script>
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">sparseMVN</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.2.2</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/example1.html">An example with a sparse Hessian</a>
    </li>
    <li>
      <a href="../articles/theory1.html">sparseMVN: The Theory</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/braunm/sparseMVN/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="theory1_files/header-attrs-2.11/header-attrs.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>sparseMVN: The Theory</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/braunm/sparseMVN/blob/master/vignettes/theory1.Rmd" class="external-link"><code>vignettes/theory1.Rmd</code></a></small>
      <div class="hidden name"><code>theory1.Rmd</code></div>

    </div>

    
    
<div id="sec:algorithms" class="section level2">
<h2 class="hasAnchor">
<a href="#sec:algorithms" class="anchor" aria-hidden="true"></a>MVN density computation and random number generation</h2>
<p>Let <span class="math inline">\(x\in\mathbb{R}^{M}\)</span> be a realization of random variable <span class="math inline">\(X\sim\mathbf{MVN}\!\left(\mu,\Sigma\right)\)</span>, where <span class="math inline">\(\mu\in\mathbb{R}^{M}\)</span> is a vector, <span class="math inline">\(\Sigma\in\mathbb{R}^{M\times M}\)</span> is a positive-definite covariance matrix, and <span class="math inline">\(\Sigma^{-1}\in\mathbb{R}^{M\times M}\)</span> is a positive-definite precision matrix.</p>
<p>The log probability density of <span class="math inline">\(x\)</span> is</p>
<p><span class="math display">\[\begin{aligned}
\log f(x)&amp;=-\frac{1}{2}\left(M \log (2\pi) + \log|\Sigma|
  +z^\top z\right),\quad\text{where}~z^\top z=\left(x-\mu\right)^\top\Sigma^{-1}\left(x-\mu\right)
 \end{aligned}\]</span></p>
<p>The two computationally intensive steps in evaluating <span class="math inline">\(\log f(x)\)</span> are computing <span class="math inline">\(\log|\Sigma|\)</span>, and <span class="math inline">\(z^\top z\)</span>, <em>without</em> explicitly inverting <span class="math inline">\(\Sigma\)</span> or repeating mathematical operations. How one performs these steps <em>efficiently</em> in practice depends on whether the covariance matrix <span class="math inline">\(\Sigma\)</span>, or the precision matrix <span class="math inline">\(\Sigma^{-1}\)</span> is available. For both cases, we start by finding a lower triangular matrix root: <span class="math inline">\(\Sigma=LL^\top\)</span> or <span class="math inline">\(\Sigma^{-1}=\Lambda\Lambda^\top\)</span>. Since <span class="math inline">\(\Sigma\)</span> and <span class="math inline">\(\Sigma^{-1}\)</span> are positive definite, we will use the Cholesky decomposition, which is the unique matrix root with all positive elements on the diagonal.</p>
<p>With the Cholesky decomposition in hand, we compute the log determinant of <span class="math inline">\(\Sigma\)</span> by adding the logs of the diagonal elements of the factors. <span class="math display">\[\begin{aligned}
  \label{eq:logDet}
  \log|\Sigma|= \begin{cases}
    \phantom{-}2\sum_{m=1}^M\log L_{mm}&amp;\text{ when $\Sigma$ is given}\\
    -2\sum_{m=1}^M\log \Lambda_{mm}&amp;\text{ when $\Sigma^{-1}$ is given}
    \end{cases}\end{aligned}\]</span></p>
<p>Having already computed the triangular matrix roots also speeds up the computation of <span class="math inline">\(z^\top z\)</span>. If <span class="math inline">\(\Sigma^{-1}\)</span> is given, <span class="math inline">\(z=\Lambda^\top(x-\mu)\)</span> can be computed efficiently as the product of an upper triangular matrix and a vector. When <span class="math inline">\(\Sigma\)</span> is given, we find <span class="math inline">\(z\)</span> by solving the lower triangular system <span class="math inline">\(Lz=x-\mu\)</span>. The subsequent <span class="math inline">\(z^\top z\)</span> computation is trivially fast.</p>
<p>The algorithm for simulating <span class="math inline">\(X\sim\mathbf{MVN}\!\left(\mu,\Sigma\right)\)</span> also depends on whether <span class="math inline">\(\Sigma\)</span> or <span class="math inline">\(\Sigma^{-1}\)</span> is given. As above, we start by computing the Cholesky decomposition of the given covariance or precision matrix. Define a random variable <span class="math inline">\(Z\sim\mathbf{MVN}\!\left(0,I_M\right)\)</span>, and generate a realization <span class="math inline">\(z\)</span> as a vector of <span class="math inline">\(M\)</span> samples from a standard normal distribution. If <span class="math inline">\(\Sigma\)</span> is given, then evaluate <span class="math inline">\(x=Lz+\mu\)</span>. If <span class="math inline">\(\Sigma^{-1}\)</span> is given, then solve for <span class="math inline">\(x\)</span> in the triangular linear system <span class="math inline">\(\Lambda^\top\left(x-\mu\right)=z\)</span>. The resulting <span class="math inline">\(x\)</span> is a sample from <span class="math inline">\(\mathbf{MVN}\!\left(\mu,\Sigma\right)\)</span>. We confirm the mean and covariance of <span class="math inline">\(X\)</span> as follows: <span class="math display">\[\begin{aligned}
 \mathbf{E}\!\left(X\right)&amp;=\mathbf{E}\!\left(LZ+\mu\right)=\mathbf{E}\!\left(\Lambda^\top Z+\mu\right)=\mu\\
   \mathbf{cov}\!\left(X\right)&amp;= \mathbf{cov}\!\left(LZ+\mu\right)=\mathbf{E}\!\left(LZZ^\top L^\top\right)=LL^\top=\Sigma\\
     \mathbf{cov}\!\left(X\right)&amp;=\mathbf{cov}\!\left(\Lambda^{\top^{-1}}Z+\mu\right)=\mathbf{E}\!\left(\Lambda^{\top^{-1}}ZZ^\top\Lambda^{-1}\right)
     =\Lambda^{\top^{-1}}\Lambda^{-1}=(\Lambda\Lambda^\top)^{-1}=\Sigma
 \end{aligned}\]</span></p>
<p>These algorithms apply when the covariance/precision matrix is either sparse or dense. When the matrix is dense, the computational complexity is <span class="math inline">\(\mathcal{O}\!\left(M^3\right)\)</span> for a Cholesky decomposition, and <span class="math inline">\(\mathcal{O}\!\left(M^2\right)\)</span> for either solving the triangular linear system or multiplying a triangular matrix by another matrix <span class="citation">(Golub and Van Loan 1996)</span>. Thus, the computational cost grows cubically with <span class="math inline">\(M\)</span> before the decomposition step, and quadratically if the decomposition has already been completed. Additionally, the storage requirement for <span class="math inline">\(\Sigma\)</span> (or <span class="math inline">\(\Sigma^{-1}\)</span>) grows quadratically with <span class="math inline">\(M\)</span>.</p>
<!-- The package calls sparse linear algebra routines that are implemented in the **CHOLMOD** library [@ChenDavis2008; @DavisHager1999; @DavisHager2009]. -->
<!-- Internally, **sparseMVN** uses standard methods of computing the MVN density and simulating MVN random variables (see Section [1.1](#sec:algorithms){reference-type="ref" reference="sec:algorithms"}). Since a large proportion of elements in the matrix are zero, we need to store only the row and column indices, and the values, of the unique nonzero elements. -->
<!-- Of course, -->
</div>
<div id="references" class="section level1">
<h1 class="hasAnchor">
<a href="#references" class="anchor" aria-hidden="true"></a>References</h1>
<!-- so $x\in\mathbb{R}^{M}$ is a realization of random variable $X\sim\mathbf{MVN}\!\left(\mu,\Sigma\right)$. These functions require the user to supply a full covariance matrix $\Sigma\in\mathbb{R}^{M\times M}$ as a base R matrix object.  The $\Sigma$ matrix is "dense" in the sense that all $M^2$ elements are stored, even though in a symmetric matrix, at most only $\binom{M+1}{2}$ of those numbers are distinct.  Internally, both rmvnorm() and dmvnorm() factor the covariance matrix using a Cholesky decomposition, whose complexity is $\mathcal{O}\!\left(M^3\right)$[@GolubVanLoan1996].[^1] This factorization is performed every time the function is called, even if the covariance matrix does not change from call to call. And then, once the covariance is factored, we still need to either multiply a triangular matrix, and solve a triangular linear system, both of which are $\mathcal{O}\!\left(M^2\right)$ [@GolubVanLoan1996] on dense matrices. Also, if the user is starting with a precision matrix $\Sigma^{-1}$, that must be inverted explicitly.
-->
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-GolubVanLoan1996" class="csl-entry">
Golub, Gene H, and Charles F Van Loan. 1996. <em>Matrix Computations</em>. 3rd ed. Johns Hopkins University Press.
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by <a href="https://braunm.github.io" class="external-link external-link">Michael Braun</a>.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link external-link">pkgdown</a> 1.6.1.9001.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
